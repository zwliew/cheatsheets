\documentclass{article}

\usepackage{../zwliew}

\title{CS5234 - Algorithms at Scale}
\author{Liew Zhao Wei}
\date{Semester 1, 2023-2024}

\begin{document}
\maketitle
\hrule

\section{Probability and Bounds}

We start by stating some crucial probability results that will be used throughout the course.

\begin{lemma}[Union Bound]
  For a countable set of events $A_1, A_2, \ldots$, we have
  \begin{align}
    \Pr\left[\bigcup_{i=1}^\infty A_i\right] \le \sum_{i=1}^\infty \Pr[A_i]
  \end{align}
\end{lemma}

\begin{lemma}[Linearity of Expectation]
  For any random variables $X_1, X_2, \ldots, X_n$, we have
  \begin{align}
    \Exp\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \Exp[X_i]
  \end{align}
\end{lemma}

\begin{lemma}[Markov's Inequality]
  For any \emph{non-negative} random variable $X$ and $t > 0$, we have
  \begin{align}
    \Pr[X \ge t] \le \frac{\Exp[X]}{t}
  \end{align}
\end{lemma}

\begin{lemma}[Chebyshev's Inequality]
  For any random variable $X$ with mean $\mu$ and variance $\sigma^2$, we have
  \begin{align}
    \Pr[|X - \mu| \ge t] \le \frac{\sigma^2}{t^2}
  \end{align}
  In fact, this holds for any moment $p$ instead of $2$.
\end{lemma}

\begin{lemma}[Chernoff-Hoeffding Bounds]
  Suppose $X_1, \ldots, X_n$ are \emph{independent} random variables with $X_i \in [0, 1]$.
  Let $X = \sum_{i=1}^n X_i$ and $\mu = \Exp[X]$ such that $\mu_L \leq \mu \leq \mu_H$.

\textbf{(Chernoff)} For any $0 \leq \delta \leq 1$,
  \begin{align}
    \Pr[X \geq (1 + \delta)\mu] \le \exp{-\frac{\delta^2\mu}{3}}
    \\
    \Pr[X \leq (1 - \delta)\mu] \le \exp{-\frac{\delta^2\mu}{2}}
    \\
    \Pr \left[ |X - \mu| \geq \delta \mu \right] \le 2 \exp{-\frac{\delta^2\mu}{3}}
    \\
  \end{align}
\textbf{(Hoeffding)} For any $\delta \geq 0$,
  \begin{align}
    \Pr[X \geq \mu + \delta] \le \exp{-\frac{2\delta^2}{n}}
    \\
    \Pr[X \leq \mu - \delta] \le \exp{-\frac{2\delta^2}{n}}
    \\
    \Pr \left[ |X - \mu| \geq \delta \right] \le 2 \exp{-\frac{2\delta^2}{n}}
  \end{align}
  More generally, if $a_i \leq X_i \leq b_i$, then
  \begin{align}
    \Pr[X \geq \mu + \delta] \le \exp{-\frac{2\delta^2}{\sum_{i=1}^n (b_i - a_i)^2}}
    \\
    \Pr[X \leq \mu - \delta] \le \exp{-\frac{2\delta^2}{\sum_{i=1}^n (b_i - a_i)^2}}
  \end{align}
\end{lemma}

\begin{definition}[$k$-Universal Hash]
  A hash family $\mathcal{H} = \set{h \colon \mathcal{U} \to S}$ is \emph{$k$-universal} if for any distinct $k$ elements $x_1, \ldots, x_k \in \mathcal{U}$ and any $k$ elements $y_1, \ldots, y_k \in S$, we have
  \begin{align}
    \Pr_{h \in_R \mathcal{H}}[h(x_1) = y_1 \land \ldots \land h(x_k) = y_k] = \frac{1}{|S|^k}
  \end{align}
  Such a hash family is also called a \emph{$k$-wise independent hash family}.
  There are constructions of $k$-universal hash families from $[n]$ to $[n]$ that take $O(k \log n)$ space.
\end{definition}

\pagebreak

\section{Simple Techniques}

\subsection{Reservoir Sampling}

We can use \emph{reservoir sampling} to uniformly sample an element from a stream without having to store the stream in advance.

\begin{algorithm}
  \caption{Reservoir Sampling algorithm}
  \begin{algorithmic}[1]
    \Procedure{ReservoirSampling}{stream $s$}
    \For{i = 1, 2, \ldots}
    \State Flip a coin with probability $1/i$
    \If{coin was heads}
    \State $x \leftarrow s_i$
    \EndIf
    \EndFor
    \State \Return $x$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

By a simple proof by induction, we can show that the probability of any element $x$ being sampled is $1/n$.

\subsection{Mean Trick to Reduce Variance}

We can take the average of multiple independent estimators to reduce the variance of the final estimator.
This is useful when the variance of the single basic estimator is too large to apply concetration bounds.

\begin{lemma}[Reduce Variance by Taking Mean of $k$ Samples]
  Let $X = \frac{1}{k} \sum_{i = 1}^{k} X_i$ where each $X_i$ has mean $\mu$ and variance $\sigma^2$.
  Then, $\Exp[X] = \mu$ and $\Var[X] = \sigma^2 / k$.
\end{lemma}

\subsection{Median Trick to Amplify Success Probability}

We can use the median of multiple independent estimators to reduce the probability of the final estimator being far from the true value (typically by Chernoff-Hoeffding bounds).

\begin{algorithm}
  \caption{Median Trick for Amplifying Sucess Probability}
  \begin{algorithmic}[1]
    \Procedure{AmplifyByMedian}{algorithm $\mathcal{A}$, integer $\ell$}
    \For{i = 1, \ldots, $\ell$}
    \State Let $Z_i$ be the output of $\mathcal{A}$ with new independent random values
    \EndFor
    \State \Return $\text{Median}(Z_1, \ldots, Z_\ell)$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

We commonly combine both the Mean and the Median tricks to form the Median of Means trick.
The typical space blow-up of such a technique is $O(\epsilon^{-2} \log (1 / \delta))$.

A slightly different version of the Mean Trick is the Min Trick, where we take the minimum instead of the median.
This is useful when there is only one-sided error -- think of the median as a two-sided error version of the min trick.
An example of this is the \emph{Count-Min-Sketch} data structure.

\section{Sketches}

1. combining sketches, linear sketches
2. Misra-Gries
3. Count-Min-Sketch and Count-Sketch

\section{Dimensions and Distances}

\begin{lemma}[Johnson-Lindenstrauss Lemma]
For any set $S \subseteq \reals^d$ of $n$-points, there is an embedding $f \colon \reals^d \to \reals^m$ for $m = O(\epsilon^{-2} \log n)$ such that
\begin{align}
  \forall u, v \in S \quad (1 - \epsilon) \|u - v\|_2^2 \le \|f(u) - f(v)\|_2^2 \le (1 + \epsilon) \|u - v\|_2^2
\end{align}
\end{lemma}
In other words, we can embed $S$ into a lower-dimensional space while approximately preserving $\ell_2$ norms.

Some observations:
\begin{itemize}
  \item The embedding has only a logarithmic dependence on $n$ and \emph{no} dependence on $d$.
  \item The embedding is can be generated using a Gaussian distribution.
  \item The embedding can be represented as a linear transformation, or in other words, a matrix.
\end{itemize}

\begin{definition}[Locality Sensitive Hash]
  A hash family $\mathcal{H} = \set{h \colon \mathcal{U} \to S}$ is a $(r_1, r_2, p_1, p_2)$-locally sensitive if for all points $p, p' \in \mathcal{U}$,

  \begin{enumerate}
    \item if $d(p, p') \le r_1$, then $\Pr_{h \in \mathcal{H}}[h(p) = h(p')] \ge p_1$,
    \item if $d(p, p') > r_2$, then $\Pr_{h \in \mathcal{H}}[h(p) = h(p')] \le p_2$.
  \end{enumerate}

\end{definition}
In other words, a \emph{locality sensitive hash} (LSH) is a hash family where similar items are more likely to collide.
Note that the definition makes sense only if $r_1 < r_2$ and $p_1 > p_2$.

WIP.
1. ANN, PLEB, how to solve them

\end{document}
