\documentclass{article}

\usepackage{../zwliew}

\title{CS5234 - Algorithms at Scale}
\author{Liew Zhao Wei}
\date{Semester 1, 2023-2024}

\begin{document}
\maketitle
\hrule

\section{Probability and Hashing}

\subsection{Probability}

We start by stating some crucial probability results that will be used throughout the course.

\begin{lemma}[Union Bound]
  For a countable set of events $A_1, A_2, \ldots$, we have
  \begin{align}
    \Pr\left[\bigcup_{i=1}^\infty A_i\right] \le \sum_{i=1}^\infty \Pr[A_i]
  \end{align}
\end{lemma}

\begin{lemma}[Linearity of Expectation]
  For any random variables $X_1, X_2, \ldots, X_n$, we have
  \begin{align}
    \Exp\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \Exp[X_i]
  \end{align}
\end{lemma}

\begin{lemma}[Markov's Inequality]
  For any \emph{non-negative} random variable $X$ and $t > 0$, we have
  \begin{align}
    \Pr[X \ge t] \le \frac{\Exp[X]}{t}
  \end{align}
\end{lemma}

\begin{lemma}[Chebyshev's Inequality]
  For any random variable $X$ with mean $\mu$ and variance $\sigma^2$, we have
  \begin{align}
    \Pr[|X - \mu| \ge t] \le \frac{\sigma^2}{t^2}
  \end{align}
  In fact, this holds for any moment $p$ instead of $2$.
\end{lemma}

\begin{lemma}[Chernoff-Hoeffding Bounds]
  Suppose $X_1, \ldots, X_n$ are \emph{independent} random variables with $X_i \in [0, 1]$.
  Let $X = \sum_{i=1}^n X_i$ and $\mu = \Exp[X]$ such that $\mu_L \leq \mu \leq \mu_H$.

\textbf{(Chernoff)} For any $0 \leq \delta \leq 1$,
  \begin{align}
    \Pr[X \geq (1 + \delta)\mu] \le \exp{-\frac{\delta^2\mu}{3}}
    \\
    \Pr[X \leq (1 - \delta)\mu] \le \exp{-\frac{\delta^2\mu}{2}}
    \\
    \Pr \left[ |X - \mu| \geq \delta \mu \right] \le 2 \exp{-\frac{\delta^2\mu}{3}}
    \\
  \end{align}
\textbf{(Hoeffding)} For any $\delta \geq 0$,
  \begin{align}
    \Pr[X \geq \mu + \delta] \le \exp{-\frac{2\delta^2}{n}}
    \\
    \Pr[X \leq \mu - \delta] \le \exp{-\frac{2\delta^2}{n}}
    \\
    \Pr \left[ |X - \mu| \geq \delta \right] \le 2 \exp{-\frac{2\delta^2}{n}}
  \end{align}
  More generally, if $a_i \leq X_i \leq b_i$, then
  \begin{align}
    \Pr[X \geq \mu + \delta] \le \exp{-\frac{2\delta^2}{\sum_{i=1}^n (b_i - a_i)^2}}
    \\
    \Pr[X \leq \mu - \delta] \le \exp{-\frac{2\delta^2}{\sum_{i=1}^n (b_i - a_i)^2}}
  \end{align}
\end{lemma}

\subsection{Hashing}

\begin{definition}[$k$-Universal Hash]
  A hash family $\mathcal{H} = \set{h \colon \mathcal{U} \to S}$ is \emph{$k$-universal} if for any distinct $k$ elements $x_1, \ldots, x_k \in \mathcal{U}$ and any $k$ elements $y_1, \ldots, y_k \in S$, we have
  \begin{align}
    \Pr_{h \in_R \mathcal{H}}[h(x_1) = y_1 \land \ldots \land h(x_k) = y_k] = \frac{1}{|S|^k}
  \end{align}
Such a hash family is also called a \emph{$k$-wise independent hash family}.
\end{definition}

\begin{lemma}[Construction of $k$-Universal Hash Family]
  There is a construction of a $k$-universal hash family from $[n]$ to $[n]$ that takes $O(k \log n)$ space.
\end{lemma}

\pagebreak

\section{Simple Techniques}

\subsection{Reservoir Sampling}

We can use \emph{reservoir sampling} to uniformly sample an element from a stream without having to store the stream in advance.

\begin{algorithm}
  \caption{Reservoir Sampling algorithm}
  \begin{algorithmic}[1]
    \Procedure{ReservoirSampling}{stream $s$}
    \For{i = 1, 2, \ldots}
    \State Flip a coin with probability $1/i$
    \If{coin was heads}
    \State $x \leftarrow s_i$
    \EndIf
    \EndFor
    \State \Return $x$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

By a simple proof by induction, we can show that the probability of any element $x$ being sampled is $1/n$.

\subsection{Mean Trick to Reduce Variance}

We can take the average of multiple independent estimators to reduce the variance of the final estimator.
This is useful when the variance of the single basic estimator is too large to apply concetration bounds.

\begin{lemma}[Reduce Variance by Taking Mean of $k$ Samples]
  Let $X = \frac{1}{k} \sum_{i = 1}^{k} X_i$ where each $X_i$ has mean $\mu$ and variance $\sigma^2$.
  Then, $\Exp[X] = \mu$ and $\Var[X] = \sigma^2 / k$.
\end{lemma}

\subsection{Median Trick to Amplify Success Probability}

We can use the median of multiple independent estimators to reduce the probability of the final estimator being far from the true value (typically by Chernoff-Hoeffding bounds).

\begin{algorithm}
  \caption{Median Trick for Amplifying Sucess Probability}
  \begin{algorithmic}[1]
    \Procedure{AmplifyByMedian}{algorithm $\mathcal{A}$, integer $\ell$}
    \For{i = 1, \ldots, $\ell$}
    \State Let $Z_i$ be the output of $\mathcal{A}$ with new independent random values
    \EndFor
    \State \Return $\text{Median}(Z_1, \ldots, Z_\ell)$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

This trick can only be used when the success probability of the original estimator is more than 50\%.
We commonly combine both the Mean and the Median tricks to form the Median of Means trick.
The typical space blow-up of such a technique is $O(\epsilon^{-2} \log (1 / \delta))$.

A slightly different version of the Mean Trick is the Min Trick, where we take the minimum instead of the median.
This is useful when there is only one-sided error -- think of the median as a two-sided error version of the min trick.
An example of this is the \emph{Count-Min-Sketch} data structure.

\pagebreak

\section{Sketches}

\begin{definition}[Sketches and Linear Sketches]
  A data structure $S$ is a \emph{sketch} if there is a space-efficient combining algorithm $\text{COMB}$ such that for any two streams $s_1$ and $s_2$, we have $S(s_1 \cdot s_2) = \text{COMB}(S(s_1), S(s_2))$.
  A sketch is \emph{linear} if $S(s_1 \cdot s_2) = S(s_1) + S(s_2)$.
\end{definition}

\subsection{Misra-Gries}

The \emph{Misra-Gries} algorithm solves the $k$-heavy hitters problem:
\begin{itemize}
\item If $x$ is a $k$-heavy hitter, then $x$ must be in the output.
\item If $x$ is not a $k$-heavy hitter, then $x$ may or may not be in the output.
\end{itemize}

\begin{algorithm}
  \caption{Misra-Gries heavy hitters algorithm}
  \begin{algorithmic}[1]
\Procedure{MisraGries}{stream $s$, integer $k$}
    \State Let $C$ be an empty hash table
    \For{i = 1, \ldots, $m$}
    \If {$a_i \in C$}
    \State $C[a_i] \leftarrow C[a_i] + 1$
    \ElsIf {$|C| < k - 1$}
    \State $C[a_i] \leftarrow 1$
    \Else
    \For{$j \in C$}
    \State $C[j] \leftarrow C[j] - 1$
    \If {$C[j] = 0$} \State Delete $C[j]$ \EndIf
    \EndFor
    \EndIf
    \EndFor
    \State \Return $\text{Keys}(C)$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{lemma}[Misra-Gries Analysis]
  The Misra-Gries algorithm with parameter $k$ uses one pass and $O(k(\log m + \log n))$ bits of space and provides, for any token $j$, an estimate $\hat{f}_j$ satisfying
  \begin{align}
    f_j - \frac{m - \hat{m}}{k} \le \hat{f}_j \le f_j
  \end{align}
\end{lemma}

As expected of a sketch, two Misra-Gries sketches can be combined by adding the counts of the same keys, sorting the counts in descending order to form a new stream of counts, and re-running the algorithm on the combined sketch.

\subsection{Count-Min-Sketch}

The \emph{Count-Min-Sketch} data structure solves the frequency estimation problem.
It takes $O(\epsilon^{-1} \log (1 / \delta) \cdot (\log m + \log n))$ space and provides, for any token $j$, an estimate $\hat{f}_j$ satisfying $f_a \leq \hat{f}_a \leq \epsilon || f_{-a} ||$ with probability at least $1 - \delta$.

\begin{algorithm}
  \caption{Count-Min-Sketch frequency estimation algorithm}
  \begin{algorithmic}[1]
    \Require $C[1 \ldots t][1 \ldots k] \leftarrow \vec{0}$, where $k = 2 / \epsilon$ and $t = \lceil{\log_2(1 / \delta)}\rceil$
    \Require Choose $t$ independent hash functions $h_1, \ldots, h_t \colon [n] \to [k]$, each from a $2$-universal family
    \Procedure{Process}{token $(j, c)$}
    \For {each token $(j, c)$ in $s$}
    \For {$i = 1, \ldots, t$}
    \State $C[i][h_i(j)] \leftarrow C[i][h_i(j)] + c$
    \EndFor
    \EndFor
    \EndProcedure
    \Procedure{Output}{query $a$}
    \State \Return $\hat{f}_a = \min_{i = 1, \ldots, t} C[i][h_i(a)]$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\pagebreak

\section{Dimensions and Distances}

\begin{lemma}[Johnson-Lindenstrauss Lemma]
For any set $S \subseteq \reals^d$ of $n$-points, there is an embedding $f \colon \reals^d \to \reals^m$ for $m = O(\epsilon^{-2} \log n)$ such that
\begin{align}
  \forall u, v \in S \quad (1 - \epsilon) \|u - v\|_2^2 \le \|f(u) - f(v)\|_2^2 \le (1 + \epsilon) \|u - v\|_2^2
\end{align}
\end{lemma}
In other words, we can embed $S$ into a lower-dimensional space while approximately preserving $\ell_2$ norms.

Some observations:
\begin{itemize}
  \item The embedding has only a logarithmic dependence on $n$ and \emph{no} dependence on $d$.
  \item The embedding is can be generated using a Gaussian distribution.
  \item The embedding can be represented as a linear transformation, or in other words, a matrix.
\end{itemize}

\begin{definition}[Locality Sensitive Hash]
  A hash family $\mathcal{H} = \set{h \colon \mathcal{U} \to S}$ is a $(r_1, r_2, p_1, p_2)$-locally sensitive if for all points $p, p' \in \mathcal{U}$,

  \begin{enumerate}
    \item if $d(p, p') \le r_1$, then $\Pr_{h \in \mathcal{H}}[h(p) = h(p')] \ge p_1$,
    \item if $d(p, p') > r_2$, then $\Pr_{h \in \mathcal{H}}[h(p) = h(p')] \le p_2$.
  \end{enumerate}

\end{definition}
In other words, a \emph{locality sensitive hash} (LSH) is a hash family where similar items are more likely to collide.
Note that the definition makes sense only if $r_1 < r_2$ and $p_1 > p_2$.

\subsection{$\epsilon$-Approximate Nearest Neighbour Problem}

\begin{definition}[$\epsilon$-Approximate Nearest Neighbour Problem]
  Given a set $P$ of points in $\reals^d$, construct a data structure, that given any point $q \in \reals^d$, returns a point $p \in P$ such that $d(p, q) \leq c \min_{p' \in P} d(p', q)$.
\end{definition}

We can reduce this problem to a simpler one called the $\epsilon$-Point Location in Equal Balls problem ($\epsilon$-PLEB).

\begin{definition}[$\epsilon$-Point Location in Equal Balls]
  Given a set $P$ of points in $\reals^d$ and radii $r_1, r_2$, construct a data structure, that given any point $q \in \reals^d$:

  \begin{enumerate}
    \item If there is a point $p \in P$ such that $d(p, q) \leq r_1$, returns \textbf{YES} and any point $p' \in P$ with $d(p', q) \leq r_2$.
    \item If there is no point $p \in P$ such that $d(p, q) \leq r_2$, returns \textbf{NO}.
  \end{enumerate}
\end{definition}

\begin{lemma}[ANN to PLEB]
  Suppose there is a data structure for $(r, (1 + \epsilon) r)-PLEB$ with space $S$ and query time $T$ for any radius $r$.
  Then, there is an algorithm for $(1 + \epsilon)^2$-ANN with space $O(S \log_{1 + \epsilon} \frac{D_{max}}{D_{min}})$ and query time $O(T \log \log_{1 + \epsilon} \frac{D_{max}}{D_{min}})$, where $D_{max}$ and $D_{min}$ denote the maximum and minimum distance between points respectively.
\end{lemma}

\begin{proof}
  Search over the radii using $(r, (1 + \epsilon) r)$-PLEB:
  \begin{align}
    \frac{D_{min}}{2}, (1+ \epsilon) \frac{D_{min}}{2}, (1 + \epsilon)^2 \frac{D_{min}}{2}, \ldots, \approx D_{max}
  \end{align}
  and binary search for the minimum radius $r$ so that the algorithm returns \textbf{YES} for a single query point $p$.
  Convince yourself that the point returned is a $(1 + \epsilon)^2$-approximate nearest neighbour to $p$.
\end{proof}

In fact, there is a more efficient reduction to reduce $(1 + \epsilon)$-ANN to $(r, (1 + \epsilon) r)$-PLEB.

Finally, we can reduce $(r, (1 + \epsilon) r)$-PLEB to $(r, (1 + \epsilon) r)$-LSH.

\begin{lemma}[PLEB to LSB]
  Suppose there is a $(r_1, r_2, p_1, p_2)$-LSH $\mathcal{H} = \set{h \colon U \to S}$.
  Then, there is an algorithm for $(r_1, r_2)$-PLEB which uses
  \begin{itemize}
    \item $O(dn + n^{1 + \rho})$ space, and
    \item $O(n^\rho)$ query time, measured in hash evaluations,
  \end{itemize}
  where $\rho = \frac{\ln 1 / p_1}{\ln 1 / p_2}$.
  This algorithm succeeds with constant probability.
\end{lemma}

\begin{proof}
  The algorithm is as follows:
  Let $k$ and $\ell$ be parameters (which we will set later). Define a new hash family $\mathcal{G}=\left\{g: U \rightarrow S^k\right\}$. Each hash function $g \in \mathcal{G}$ takes the form $g(p)=\left(h_1(p), h_2(p), \ldots, h_k(p)\right)$, where each hash function $h_i$ is in $\mathcal{H}$.

  First, preprocessing:
  \begin{enumerate}
    \item Choose hash functions $g_1, \ldots, g_{\ell}$ from $\mathcal{G}$.
    \item For each $p$ in the given set of points $P$, store $p$ in each of the buckets specified by $g_1(p), \ldots, g_{\ell}(p)$.
    \item Discard all empty buckets.
  \end{enumerate}

  Now, when queried with the point $q$, we search through the buckets $g_1(q), \ldots, g_{\ell}(q)$, and stop after the first $2 \ell$ points. If any of these points $p$ has $d(p, q) \leq r_2$, return $p$ with \textbf{YES}; otherwise, return \textbf{NO}.

  Convince yourself that this algorithm achieves the desired guarantees.
\end{proof}

\end{document}
