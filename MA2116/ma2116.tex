\documentclass{article}

\usepackage{../zwliew}

\title{MA2116 - Probability}
\author{Liew Zhao Wei}
\date{Semester 1, 2023-2024}

\begin{document}
\maketitle

\begin{abstract}
  This is a set of incomplete notes for an introductory course on probability.
\end{abstract}

\hrule

\section{Preliminaries}

\subsection{Taylor series expansions}

We should know the following Taylor series expansions by heart:
\begin{align*}
  e^x        & = \sum\limits_{n = 0}^\infty \frac{x^n}{n!}
  \\
  \ln(1 + x) & = \sum\limits_{n = 1}^\infty (-1)^{n + 1} \frac{x^n}{n}
  \\
  \sin x     & = \sum\limits_{n = 0}^\infty (-1)^n \frac{x^{2n + 1}}{(2n + 1)!}
  \\
  \cos x     & = \sum\limits_{n = 0}^\infty (-1)^n \frac{x^{2n}}{(2n)!}
\end{align*}

\subsection{Combinatorics}

Here are some useful combinatorial identities.

\[
  \text{Binomial theorem: } (x + y)^n = \sum\limits_{k = 0}^n \binom{n}{k} x^k y^{n - k}
\]

\[
  i \binom{n}{i} = n \binom{n - 1}{i - 1}
\]

\subsection{Calculus}

The following theorem in calculus will be important for studying continuous random variables.

\begin{theorem}[First Fundamental Theorem of Calculus]
  Let $f$ be continuous on $[a, b]$ and define $F$ on $[a, b]$ by
  \[
    F(x) = \int_a^x f(t) \, dt.
  \]
  Then, $F$ is differentiable on $(a, b)$ and
  \[
    F'(x) = f(x)
  \]
  for all $x$ in $(a, b)$.
\end{theorem}

\section{Axioms of Probability}

TODO.

\section{Discrete random variables}

Let's start simple -- with discrete random variables.
A random variable $X$ is \emph{discrete} if it takes values in a countable set.
$p_X(a) = \Pr(X = a)$ is the \emph{probability mass function} of $X$.
$F_X(a) = \Pr(X \leq a) = \sum_{x \leq a} p(x)$ is the \emph{cumulative distribution function} of $X$.

\subsection{Expectation}

When given a random variable, we are often interested in its \emph{average} value.
This is captured by the notion of \emph{expectation}.

\begin{definition}[Expected value of a discrete r.v.]
  Let $X$ be a discrete random variable.
  Then the \emph{expected value}, or \emph{expectation}, of $X$, denoted by $\Exp[X]$, is defined as
  \[
    \Exp[X] = \sum_{x \in \Omega_X} x \Pr(X = x).
  \]
\end{definition}

We also have the following intuitive results for the expectated value of a discrete random variable.

\begin{proposition}[Law of the unconscious statistician]
  Let $X$ be a discrete random variable and $g$ be real-valued a function.
  Then
  \[
    \Exp[g(X)] = \sum_{x \in \Omega_X} g(x) \Pr(X = x).
  \]
\end{proposition}

\begin{corollary}
  Let $X$ be a discrete random variable and $a, b \in \reals$.
  Then
  \[
    \Exp[a X + b] = a \Exp[X] + b.
  \]
\end{corollary}

\begin{proposition}[Alternative formula for expectation of nonnegative discrete r.v.]
  Let $X$ be a \emph{nonnegative} discrete random variable and $g$ be real-valued a function.
  Then
  \[
    \Exp[X] = \sum_{x = 0} \Pr(X > x).
  \]
\end{proposition}

One important property of expectations of (general) random variables is that it is \emph{linear}.

\begin{theorem}{Linearity of expectation}
  Let $X_1, X_2, \ldots, X_n$ be random variables (not necessarily discrete).
  Then
  \[
    \Exp\left[\sum\limits_{i = 1}^n X_i\right] = \sum\limits_{i = 1}^n \Exp[X_i].
  \]
\end{theorem}

Another useful result to use is the \emph{law of total expectation}.

\begin{theorem}[Law of total expectation]
  Let $X$ be a discrete random variable and $Y$ be a discrete random variable that depends on $X$.
  Then
  \[
    \Exp[Y] = \Exp[\Exp[Y \mid X]] = \sum_{x \in \Omega_X} \Exp[Y \mid X = x] \Pr(X = x).
  \]
\end{theorem}

\subsection{Variance}

Expectation alone doesn't reveal much about the distribution of a random variable.
For example, compare these two slot machines:
\begin{enumerate}
  \item The first slot machine gives you \$100 with probability $1$.
  \item The second slot machine gives you \$10000 with probability $0.01$ and \$0 with probability $0.99$.
\end{enumerate}
The expected payout of both slot machines is \$100, but the chances of winning are very different.
This difference is captured by the notion of \emph{variance}.

\begin{definition}[Variance of a discrete r.v.]
  Let $X$ be a discrete random variable.
  Then the \emph{variance} of $X$, denoted by $\Var(X)$, is defined as
  \[
    \Var(X) = \Exp[(X - \Exp[X])^2].
  \]
\end{definition}

\begin{remark}[Alternative formula]
  The variance can also be computed using the following formula:
  \[
    \Var(X) = \Exp[X^2] - \Exp[X]^2.
  \]
\end{remark}

\begin{remark}
  Since variance is non-negative, we have
  \[
    \Exp[X^2] \geq \Exp[X]^2.
  \]
  This can be used to prove the \emph{friendship paradox}.
\end{remark}

\begin{proposition}[Variance of linear combination]
  Let $X$ be a discrete random variable and $a, b \in \reals$.
  Then
  \[
    \Var(a X + b) = a^2 \Var(X).
  \]
\end{proposition}

\subsection{Bernoulli and binomial random variables}

Perhaps the most basic and well-known random variable is the \emph{binomial} random variable and its little brother, the \emph{Bernoulli} random variable.
We motivate this with the age old example of a coin toss.

\begin{example}[Coin toss]
  Suppose we toss a coin $n$ times that comes up heads with probability $p$ and tails with probability $1 - p$.
  Let $X$ be the number of heads that appear.
  Then $X$ is a \emph{binomial} random variable with parameters $n$ and $p$.
  It takes values in $\{0, 1, \ldots, n\}$.
\end{example}

\begin{definition}[Binomial random variable]
  A random variable $X$ is said to be \emph{binomial} with parameters $n$ and $p$, denoted by $X \sim \Bin(n, p)$, if $X$ has the following probability mass function:
  \[
    \Pr(X = i) = \binom{n}{i} p^i (1 - p)^{n - i}, \quad i = 0, 1, \ldots, n.
  \]
\end{definition}

\begin{remark}[Bernoulli random variable]
  A \emph{Bernoulli} random variable is a special case of a binomial random variable with $n = 1$.
\end{remark}

When $n = 1$, we can easily compute that $\Exp[X] = p$ and $\Var(X) = p(1 - p)$.
By the additivity of expectation and variance, it is also easy to compute the expectation and variance for general $n$.
\begin{theorem}[Properties of binomial random variables]
  Let $X \sim \Bin(n, p)$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = np$
    \item $\Var(X) = np(1 - p)$
    \item $\Exp[X^k] = np \Exp[(Y + 1)^{k - 1}]$ where $Y \sim \Bin(n - 1, p)$
  \end{enumerate}
\end{theorem}

\begin{proof}
  A proof of $\Exp[X^k]$ is done with some algebraic manipulation of the definition of $\Exp[X^k]$ and using the identity
  \[
    i \binom{n}{i} = n \binom{n - 1}{i - 1}
  \]
\end{proof}

The following relationship is useful for computing the distribution function of a binomial random variable.
It can also be used to find the maximum of the probability mass function.

\begin{proposition}[Relationship between $\Pr(X = k + 1)$ and $\Pr(X = k)$]
  Let $X \sim \Bin(n, p)$. Then
  \[
    \Pr(X = k + 1) = \frac{p}{1 - p} \frac{n - k}{k + 1} \Pr(X = k).
  \]
\end{proposition}

\begin{corollary}[Condition for monotonic increase]
  Let $X \sim \Bin(n, p)$.
  Then, as $k$ goes from $0$ to $n$, $\Pr(X = k)$ first increases monotonically and then decreases monotonically.
  In fact, $\Pr(X = k) \geq \Pr(X = k + 1)$ if and only if $k \leq (n + 1)p$.
\end{corollary}

\subsection{Poisson random variable}

Besides the binomial random variable, another important discrete random variable is the \emph{Poisson} random variable.

\begin{definition}[Poisson random variable]
  A random variable $X$ is said to be \emph{Poisson} with parameter $\lambda > 0$, denoted by $X \sim \Pois(\lambda)$, if $X$ has the following probability mass function:
  \[
    \Pr(X = i) = e^{-\lambda} \frac{\lambda^i}{i!}, \quad i = 0, 1, 2, \ldots.
  \]
\end{definition}
Observe that the $i$th term is $e^{-\lambda}$ times the $i$th term of the Taylor series expansion of $e^\lambda$.

\begin{remark}
  By the Taylor series expansion of $e^x$, we have
  \[
    \sum_{i = 0}^\infty \Pr(X = i) = e^{-\lambda} \sum_{i = 0}^\infty \frac{\lambda^i}{i!} = e^{-\lambda} e^\lambda = 1.
  \]
\end{remark}

The Poisson random variable is useful for approximating the binomial random variable when $n$ is large and $p$ is small enough so that $\lambda = np$ is of moderate size.

\begin{example}[Examples that can be approximated by a Poisson r.v.]
  We can think of each of these examples as a long series of independent trial, each with a small chance of success.
  Thus, we can approximate the number of successes by a Poisson random variable.
  \begin{enumerate}
    \item The number of misprints on a page of a book.
    \item The number of people in a community who survive to age 100.
    \item The number of wrong telephone numbers dialed in a day.
    \item The number of packages of biscuits sold in a store in a day.
    \item The customers centering a post office on a given day.
  \end{enumerate}
\end{example}

\begin{remark}
  This approximation works decently even with $n$ as small as $10$ and $p$ as large as $0.1$.
  For example, with $X \sim \Bin(10, 0.1)$ and $Y \sim \Pois(1)$, we have
  \[
    \Pr(X \leq 1) \approx 0.7361
  \]
  \[
    \Pr(Y \leq 1) \approx 0.7358
  \]
\end{remark}

\begin{theorem}[Properties of Poisson r.v.]
  Let $X \sim \Pois(\lambda)$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = \lambda$
    \item $\Var(X) = \lambda$
  \end{enumerate}
\end{theorem}
These properties should be intuitive by comparing Poisson random variables with binomial random variables, but with large $n$.

We have shown that the Poisson distribution approximates the binomial distribution when $n$ is large and $p$ is small.
In fact, we do not need the events be independent or to occur with equal probability.
We just need the probability of the events to be small and the events to be at most weakly dependent.
This is explained in what we call the \emph{Poisson paradigm}.

\begin{definition}[Weak dependence]
  Events $E$ and $F$ are weakly dependent if $\Pr(E) \approx \Pr(E \mid F)$.
\end{definition}

\begin{theorem}[Poisson Paradigm]
  Consider $n$ events where the $i$th event occurs with probability $p_i$.
  If all the $p_i$ are small and the trials are either independent or at most weakly dependent, then the number of events that occur approximately has a Poisson distribution with mean $\lambda = \sum_{i = 1}^n p_i$.
\end{theorem}

\begin{proposition}[Relationship between $\Pr(X = k + 1)$ and $\Pr(X = k)$]
  Let $X \sim \Pois(\lambda)$.
  Then
  \[
    \Pr(X = k + 1) = \frac{\lambda}{k + 1} \Pr(X = k).
  \]
\end{proposition}

One other use of the Poisson distribution is in \emph{poisson processes}.

\begin{theorem}[Poisson process]
  Suppose events occur at random points in time, and let $N(t)$ denote the number of events that occur in an interval of length $t$.
  $N(t)$ is a \emph{Poisson process} having rate $\lambda > 0$ if
  \begin{enumerate}
    \item The process begins at time $0$, that is, $N(0) = 0$.
    \item $\Pr(N(h) = 1) = \lambda h + o(h)$
    \item $\Pr(N(h) \geq 2) = o(h)$
    \item The number of events occurring in disjoint time intervals are independent (otherwise called the independent increment assumption).
    \item The distribution of the number of events occurring in a time interval of length $t$ depends only on the length of the interval and not on its location (otherwise called the stationary increment assumption).
  \end{enumerate}
  $N(t)$ has a Poisson distribution with mean $\lambda t$.
\end{theorem}

\begin{example}[Examples of poisson processes]
  Poisson processes can approximate phonemona like:
  \begin{enumerate}
    \item The number of earthquakes occurring in a fixed time span.
    \item The number of wars per year.
    \item The number of electrons emitted from a heated cathode during a fixed time period.
    \item The number of deaths, in a given period of time, of the policyholders of a life insurance company.
  \end{enumerate}
\end{example}

\subsection{Geometric random variable}

Imagine tossing a coin repeatedly until we get a head.
Let $X$ be the number of tosses required.
Then $X$ is a \emph{geometric} random variable.

\begin{definition}[Geometric random variable]
  A random variable $X$ is said to be \emph{geometric} with parameter $p$ if $X$ has the following probability mass function:
  \[
    \Pr(X = i) = (1 - p)^{i - 1} p, \quad i = 1, 2, \ldots.
  \]
\end{definition}

\begin{remark}
  By the geometric series, we have
  \[
    \sum\limits_{i = 1}^\infty \Pr(X = i) = p \sum\limits_{i = 1}^\infty (1 - p)^{i - 1} = 1
  \]
\end{remark}

\begin{proposition}[Properties of geometric r.v.]
  Let $X$ be a geometric random variable with parameter $p$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = 1/p$
    \item $\Var(X) = (1 - p)/p^2$
  \end{enumerate}
\end{proposition}
The expectation should be unsurprising.
The variance should also be somewhat intuitive since
\begin{enumerate}
  \item When $p$ is small, $X$ is approximately an exponential random variable with $\lambda = p$ and so $\Var(X) \approx 1/p^2$.
  \item When $p$ is large, $\Var(X) \approx 0$.
\end{enumerate}

\begin{remark}
  The exponential distribution is the continuous analogue of the geometric distribution.
\end{remark}

\subsection{Negative binomial random variable}

Suppose we toss a coin repeatedly until we get $r$ heads.
Let $X$ be the number of tosses required.
Then $X$ is a \emph{negative binomial} random variable.

\begin{definition}[Negative binomial random variable]
  A random variable $X$ is said to be \emph{negative binomial} with parameters $r$ and $p$ if $X$ has the following probability mass function:
  \[
    \Pr(X = i) = \binom{i - 1}{r - 1} p^r (1 - p)^{i - r}, \quad i = r, r + 1, \ldots.
  \]
\end{definition}

\begin{remark}
  A geometric random variable is a special case of a negative binomial random variable with $r = 1$.
\end{remark}

The following properties should not be surprising since the negative binomial random variable is a sum of $r$ independent geometric random variables.

\begin{proposition}[Properties of negative binomial r.v.]
  Let $X$ be a negative binomial random variable with parameters $r$ and $p$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = r/p$
    \item $\Var(X) = r(1 - p)/p^2$
  \end{enumerate}
\end{proposition}


\subsection{Hypergeometric random variable}

Suppose we randomly choose $n$ balls from an urn containing $m$ white balls and $N - m$ black balls without replacement.
Let $X$ be the number of white balls chosen.
Then $X$ is a \emph{hypergeometric} random variable.

\begin{definition}[Hypergeometric random variable]
  A random variable $X$ is said to be \emph{hypergeometric} with parameters $n$, $N$, and $m$ if $X$ has the following probability mass function:
  \[
    \Pr(X = i) = \frac{\binom{m}{i} \binom{N - m}{n - i}}{\binom{N}{n}}, \quad i = 0, 1, \ldots, n.
  \]
\end{definition}

\begin{proposition}[Properties of hypergeometric r.v.]
  Let $X$ be a hypergeometric random variable with parameters $n$, $N$, and $m$.
  Let $p = m/N$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = np$
    \item $\Var(X) = np(1 - p) \left(1 - \dfrac{n - 1}{N - 1} \right)$
  \end{enumerate}
\end{proposition}
The following remark is intuitive since binomial is like hypergeometric, but with replacement.
\begin{remark}
  When $N \gg n$, $X$ is approximately a binomial random variable with parameters $n$ and $p$.
\end{remark}

\section{Continuous random variables}

Now, onto something more interesting -- continuous random variables.
A random variable $X$ is \emph{continuous} if there exists a non-negative function $f$ such that for any measurable set $B$<
\[
  \Pr(X \in B) = \int_B f(x) \, dx
\]
$f$ is called the \emph{probability density function} of $X$.
Observe that it is meaningless to discuss the probability of $X$ taking a particular value.

The cumulative distribution function $F_X$ of $X$ is defined as
\[
  F_X(a) = \Pr(X \leq a) = \int_{-\infty}^a f(x) \, dx = \Pr(X < a)
\]
It is clear that the density is the derivative of the cumulative distribution function.
An alternative intuitive interpretation is that $f(a)$ is how likely $X$ will be near $a$, as given by
\[
  \Pr(a - \dfrac{\epsilon}{2} < X < a + \dfrac{\epsilon}{2}) = \int_{a - \epsilon/2}^{a + \epsilon/2} f(x) \, dx \approx \epsilon f(a)
\]
when $\epsilon$ is small and $f(\cdot)$ is continuous at $x = a$.

\subsection{Expectation}

Recall that the expected value of a discrete random variable is defined as
\[
  \Exp[X] = \sum_{x \in \Omega_X} x \Pr(X = x).
\]
We have an analogous definition of expectation for continuous random variables.
\begin{definition}[Expected value of a continuous r.v.]
  Let $X$ be a continuous random variable.
  Then the \emph{expected value}, or \emph{expectation}, of $X$, denoted by $\Exp[X]$, is defined as
  \[
    \Exp[X] = \int_{-\infty}^\infty x f(x) \, dx.
  \]
\end{definition}
This is intuitive since we can imagine partitioning the domain of $x$ into small subintervals $dx$, and for small $dx$, we have
\[
  f(x) \, dx \approx \Pr(x \leq X \leq x + dx)
\]

Just as in the discrete case, we have the following results for continuous random variables.

\begin{proposition}[Law of the unconscious statistician]
  Let $X$ be a continuous random variable and $g$ be real-valued a function.
  Then
  \[
    \Exp[g(X)] = \int_{-\infty}^\infty g(x) f(x) \, dx.
  \]
\end{proposition}

\begin{corollary}
  Let $X$ be a continuous random variable and $a, b \in \reals$.
  Then,
  \[
    \Exp[aX + b] = a \Exp[X] + b
  \]
\end{corollary}

\begin{lemma}[Alternative formula for expectation of nonnegative continuous r.v.]
  Let $Y$ be a \emph{nonnegative} continuous random variable.
  Then,
  \[
    \Exp[Y] = \int_{0}^\infty \Pr(Y > y) \, dy.
  \]
\end{lemma}

By a similar proof, the \emph{law of total expectation} holds for continuous random variables.

\begin{theorem}[Law of total expectation]
  Let $X$ be a continuous random variable and $Y$ be a continuous random variable that depends on $X$.
  Then
  \[
    \Exp[Y] = \Exp[\Exp[Y \mid X]] = \int_{-\infty}^\infty \Exp[Y \mid X = x] f(x) \, dx.
  \]
\end{theorem}

\subsection{Variance}

Variance for continuous random variables is defined exactly as it is in the discrete case.

\begin{definition}[Variance of a continuous r.v.]
  Let $X$ be a continuous random variable.
  Then the \emph{variance} of $X$, denoted by $\Var(X)$, is defined as
  \[
    \Var(X) = \Exp[(X - \Exp[X])^2].
  \]
\end{definition}

We restate the same results for variance as in the discrete case.
They are proved similarly.

\begin{remark}[Alternative formula]
  The variance can also be computed using the following formula:
  \[
    \Var(X) = \Exp[X^2] - \Exp[X]^2.
  \]
\end{remark}

\begin{remark}
  Since variance is non-negative, we have
  \[
    \Exp[X^2] \geq \Exp[X]^2.
  \]
\end{remark}

\begin{proposition}[Variance of linear combination]
  Let $X$ be a discrete random variable and $a, b \in \reals$.
  Then
  \[
    \Var(a X + b) = a^2 \Var(X).
  \]
\end{proposition}

\subsection{Uniform random variable}

The simplest continuous random variable is the \emph{uniform} random variable.

\begin{definition}[Uniform random variable]
  A random variable $X$ is said to be \emph{uniform} on the interval $[a, b]$ if $X$ has the following probability density function:
  \[
    f(x) = \begin{cases}
      \dfrac{1}{b - a} & \text{if } a \leq x \leq b \\
      0                & \text{otherwise}
    \end{cases}
  \]
  The cumulative distribution function of $X$ is
  \[
    F_X(x) = \begin{cases}
      0                    & \text{if } x \leq a  \\
      \dfrac{x - a}{b - a} & \text{if } a < x < b \\
      1                    & \text{if } x \geq b
    \end{cases}
  \]
\end{definition}

\begin{proposition}[Properties of uniform r.v.]
  Let $X$ be a uniform random variable on the interval $(a, b)$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = \dfrac{a + b}{2}$
    \item $\Var(X) = \dfrac{(b - a)^2}{12}$
  \end{enumerate}
\end{proposition}

\begin{remark}
  Given the uniform distribution $X$ over $(0, 1)$, we can compute a uniform distribution over $(a, b)$ using the transformation $Y = a + (b - a) X$.
\end{remark}

\subsection{Normal random variable}

The most widely used continuous random variable is the \emph{normal} random variable.

\begin{definition}[Normal random variable]
  A random variable $X$ is said to be \emph{normal} with parameters $\mu$ and variance $\sigma^2$ if $X$ has the following probability density function:
  \[
    f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}
  \]
  The cumulative distribution function of $X$ is
  \[
    F_X(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^x e^{-\frac{(t - \mu)^2}{2 \sigma^2}} \, dt
  \]
\end{definition}

\begin{remark}
  Passing to two dimensions and using polar coordinates, we have
  \[
    \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}} \, dx = \sqrt{2 \pi}
  \]
  which helps us show that $f$ is a probability density function.
\end{remark}

\begin{definition}[Standard normal random variable]
  It is customary to call the normal random variable with parameters $\mu = 0$ and $\sigma^2 = 1$ the \emph{standard normal} random variable.
  It is denoted as $Z \sim N(0, 1)$.
  The cumulative distribution function of $Z$ is denoted by $\Phi$.
\end{definition}

\begin{remark}
  For any $x$, we have
  \[
    \Phi (-x) = 1 - \Phi(x)
  \]
\end{remark}

\begin{remark}
  Given the standard normal distribution $Z$, we can compute a normal distribution with mean $\mu$ and variance $\sigma^2$ using the transformation $Y = \mu + \sigma Z$.
\end{remark}

\begin{proposition}[Properties of normal r.v.]
  Let $X$ be a normal random variable with parameters $\mu$ and $\sigma^2$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = \mu$
    \item $\Var(X) = \sigma^2$
  \end{enumerate}
\end{proposition}

Recall that we can approximate a binomial random variable with a Poisson random variable when $n$ is large and $p$ is small so that $\lambda = np$ is of moderate size.
Similarly, the following result shows that we can approximate a binomial random variable with a normal random variable when $np(1 - p)$ is large (typically at least $10$).

\begin{theorem}[De Moivre-Laplace limit theorem]
  Let $S_n \sim \Bin(n, p)$ and $X \sim N(np, np(1 - p))$.
  Then, for any $a < b$,
  \[
    \Pr(a \leq S_n \leq b) \to \Pr(a \leq X \leq b)
  \]
  as $n \to \infty$.
\end{theorem}
This result is a consequence of the central limit theorem, which we will see later.

\begin{remark}
  Let $S_n \sim \Bin(n, p)$ and $X \sim N(np, np(1 - p))$.
  Then, $P(S_n = i)$ is approximated by $P(i - 0.5 \leq X \leq i + 0.5)$.
\end{remark}

\subsection{Exponential random variable}

Consider a Poisson process $N(t)$ with rate $\lambda$.
If $X$ is the waiting time until the first event occurs, then
\[
  P(X > t) = P(N(t) = 0) = e^{-\lambda t}
\]
We call $X$ an \emph{exponential} random variable with parameter $\lambda$.
It is the continuous analogue of the geometric random variable.

\begin{definition}[Exponential random variable]
  A random variable $X$ is said to be \emph{exponential} with parameter $\lambda > 0$ if $X$ has the following probability density function:
  \[
    f(x) = \begin{cases}
      \lambda e^{-\lambda x} & \text{if } x \geq 0 \\
      0                      & \text{otherwise}
    \end{cases}
  \]
  The cumulative distribution function of $X$ is
  \[
    F_X(x) = \begin{cases}
      1 - e^{-\lambda x} & \text{if } x \geq 0 \\
      0                  & \text{otherwise}
    \end{cases}
  \]
\end{definition}

Just like geometric distributions, the exponential distribution are memoryless.
In fact, it is the only continuous distribution that is memoryless.
\begin{definition}[Memorylessness]
  A random variable $X$ is said to be \emph{memoryless} if for all $s, t \geq 0$,
  \[
    \Pr(X > s + t \mid X > s) = \Pr(X > t)
  \]
\end{definition}

\begin{remark}
  Given an exponential distribution $X$ with parameter $1$, we can compute an exponential distribution with parameter $\lambda$ using the transformation $Y = \lambda X$.
\end{remark}

\begin{proposition}[Properties of exponential r.v.]
  Let $X$ be an exponential random variable with parameter $\lambda$.
  Then
  \begin{enumerate}
    \item $\Exp[X^n] = \frac{n}{\lambda} \Exp[X^{n - 1}]$
    \item $\Exp[X] = 1/\lambda$
    \item $\Var(X) = 1/\lambda^2$
  \end{enumerate}
\end{proposition}

\subsection{Laplace random variable}

The \emph{Laplace} random variable a variation of the exponential random variable that is equally likely to be positive or negative, and whose absolute value is exponentially distributed with parameter $\lambda \geq 0$.

\begin{definition}[Laplace random variable]
  A random variable $X$ is said to be \emph{Laplace} or \emph{double exponential} with parameter $\lambda > 0$ if $X$ has the following probability density function:
  \[
    f(x) = \dfrac{\lambda}{2} e^{-\lambda |x|}
  \]
  The cumulative distribution function of $X$ is
  \[
    F_X(x) = \begin{cases}
      \dfrac{1}{2} e^{-\lambda x}     & \text{if } x < 0    \\
      1 - \dfrac{1}{2} e^{-\lambda x} & \text{if } x \geq 0
    \end{cases}
  \]
\end{definition}

\begin{remark}
  If $X$ is a Laplace random variable with parameter $\lambda$, then $|X|$ is an exponential random variable with parameter $\lambda$.
\end{remark}

\subsection{Hazard rate function}

\begin{definition}[Hazard rate function]
  Let $X$ be a continuous random variable denoting the lifetime of an item, with cumulative distribution function $F$ and probability density function $f$.
  Then the \emph{hazard rate function} of $X$ is defined as
  \[
    h(x) = \frac{f(x)}{1 - F(x)}
  \]
  It represents the instantaneous probability intensity of failure at time $x$, given that the object has survived up to time $x$.
\end{definition}
Since the exponential distribution is memoryless, we should verify that its hazard rate function is constant.

It turns out that the hazard rate function uniquely determines the distribution function $F$.
\begin{remark}
  Let $\lambda(s), s \geq 0$ be the hazard rate function of a non-negative random variable $X$.
  Then,
  \[
    F(x) = 1 - \exp\left(-\int_0^x \lambda(s) \, ds\right)
  \]
\end{remark}

\subsection{Gamma random variable}

A \emph{Gamma} random variable represents the time until the $n$th event in a Poisson process with rate $\lambda$.
It is a generalization of the exponential random variable.

\begin{definition}[Gamma random variable]
  A random variable $X$ is said to be \emph{gamma} with parameters $\alpha > 0$ and $\lambda > 0$ if $X$ has the following probability density function:
  \[
    f(x) = \begin{cases}
      \dfrac{\lambda e^{-\lambda x} (\lambda x)^{\alpha - 1}}{\Gamma(\alpha)} & \text{if } x \geq 0 \\
      0                                                                       & \text{otherwise}
    \end{cases}
  \]
  The cumulative distribution function of $X$ is
  \[
    F_X(x) = \begin{cases}
      \dfrac{\gamma(\alpha, \lambda x)}{\Gamma(\alpha)} & \text{if } x \geq 0 \\
      0                                                 & \text{otherwise}
    \end{cases}
  \]
  where $\gamma(\alpha, \lambda x)$ is the lower incomplete gamma function.
\end{definition}
Note that $\alpha$ does not need to be an integer, but if when it is, we have $\Gamma(\alpha) = (\alpha - 1)!$.

\begin{remark}[$n$-Erlang distribution]
  Let $n = \alpha$ be an integer.
  Then, the Gamma distribution with parameters $(n, \lambda)$ is known as the \emph{$n$-Erlang} distribution.
\end{remark}

Given the similarities between the Gamma and exponential distributions, the following properties should not be surprising.
\begin{proposition}[Properties of gamma r.v.]
  Let $X$ be a gamma random variable with parameters $\alpha$ and $\lambda$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = \dfrac{\alpha}{\lambda}$
    \item $\Var(X) = \dfrac{\alpha}{\lambda^2}$
  \end{enumerate}
\end{proposition}

\subsection{Beta random variable}

\begin{definition}[Beta function]
  The \emph{beta function} is defined as
  \[
    B(\alpha, \beta) = \int_0^1 x^{\alpha - 1} (1 - x)^{\beta - 1} \, dx = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}
  \]
\end{definition}

A beta random variable represents the probability density of attaining $\alpha - 1$ successes and $\beta - 1$ failures in $\alpha + \beta - 2$ Bernoulli trials with probability $x$.

\begin{definition}[Beta random variable]
  A random variable $X$ is said to be \emph{beta} with parameters $\alpha > 0$ and $\beta > 0$ if $X$ has the following probability density function:
  \[
    f(x) = \begin{cases}
      \dfrac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha, \beta)} & \text{if } 0 \leq x \leq 1 \\
      0                                                            & \text{otherwise}
    \end{cases}
  \]
  The cumulative distribution function of $X$ is
  \[
    F_X(x) = \begin{cases}
      \dfrac{B(x; \alpha, \beta)}{B(\alpha, \beta)} & \text{if } 0 \leq x \leq 1 \\
      0                                             & \text{otherwise}
    \end{cases}
  \]
  where $B(\alpha, \beta)$ is the beta function.
\end{definition}

\begin{proposition}[Properties of beta r.v.]
  Let $X$ be a beta random variable with parameters $\alpha$ and $\beta$.
  Then
  \begin{enumerate}
    \item $\Exp[X] = \dfrac{\alpha}{\alpha + \beta}$
    \item $\Var(X) = \dfrac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$
  \end{enumerate}
\end{proposition}

\subsection{Distributions of functions of random variables}

When composing functions of continuous random variables, we want to be able to compute the distribution of the resulting random variable.
There are some techniques we can use.

\begin{theorem}
  Let $X$ be a continuous random variable having probability density funciton $f_X$.
  Suppose that $g(x)$ is a strictly monotonic (increasing or decreasing), differentiable function of $x$.
  Then, the random variable $Y$ defined by $Y = g(X)$ has probability density function
  \[
    f_Y(y) = \begin{cases}
      f_X(g^{-1}(y)) \left| \dfrac{d}{dy} g^{-1}(y) \right| & \text{if } g^{-1}(y) \text{ exists} \\
      0                                                     & \text{otherwise}
    \end{cases}
  \]
\end{theorem}

\section{Jointly Distributed Random Variables}

We are often interested in probability statements concerning multiple random variables.
Let's define some terms for this.

\begin{definition}[Joint cumulative probability distribution function]
  Let $X$ and $Y$ be random variables.
  The \emph{joint cumulative probability distribution function} of $X$ and $Y$ is defined as
  \[
    F(a, b) = \Pr(X \leq a, Y \leq b)
  \]
\end{definition}
Any joint probabiltiy statement can be answered in terms of $F$.
For example,
\[
  \Pr(a_1 \leq X \leq a_2, b_1 \leq Y \leq b_2) = F(a_2, b_2) + F(a_1, b_1) - F(a_1, b_2) - F(a_2, b_1)
\]

When $X$ and $Y$ are discrete random variables, it is convenient to define their \emph{joint probability mass function}.

\begin{definition}[Joint probability mass function]
  Let $X$ and $Y$ be discrete random variables.
  The \emph{joint probability mass function} of $X$ and $Y$ is defined as
  \[
    p(x, y) = \Pr(X = x, Y = y)
  \]
\end{definition}
The probability mass function of $X$ is then given by
\[
  p_X(x) = \sum_j p(x, y_j)
\]

Similarly, when $X$ and $Y$ are continuous, we can define their \emph{joint probability density function}.

\begin{definition}[Jointly continuous]
  Let $X$ and $Y$ be continuous random variables.
  Then $X$ and $Y$ are said to be \emph{jointly continuous} if there exists a function $f(x, y)$ such that for any set $C$ of pairs $(x, y)$,
  \[
    \Pr((X, Y) \in C) = \iint_C f(x, y) \, dx \, dy
  \]
  $f$ is called the \emph{joint probability density function} of $X$ and $Y$.
\end{definition}

\begin{remark}
  In particular, we have
  \[
    F(a, b) = \int_{-\infty}^b \int_{-\infty}^a f(x, y) \, dx \, dy
  \]
  \[
    f(a, b) = \frac{\partial^2}{\partial a \partial b} F(a, b)
  \]
\end{remark}

\begin{remark}
  Based on the derived equation below, we can also interpret $f(a, b)$ as the probability that $(X, Y)$ lies near $(a, b)$.
  \[
    \Pr(a \leq X \leq a + \Delta a, b \leq Y \leq b + \Delta b) \approx f(a, b) \Delta a \Delta b
  \]
\end{remark}

\begin{remark}
  The probability density function of $X$ is given by
  \[
    \Pr(X \in A) = \int_A \int_{-\infty}^\infty f(x, y) \, dy \, dx = \int_A f_X(x) \, dx
  \]
\end{remark}

Generalizing these definitions to $n$ random variables, we have the following definitions.

\begin{definition}[Definitions for $n$ joint r.v.s]
  Let $X_1, X_2, \ldots, X_n$ be random variables.
  The joint cumulative probability distribution function is
  \[
    F(a_1, a_2, \ldots, a_n) = \Pr(X_1 \leq a_1, X_2 \leq a_2, \ldots, X_n \leq a_n)
  \]
  They are \emph{jointly continuous} if there exists a function $f(x_1, x_2, \ldots, x_n)$ such that for any set $C$ of pairs $(x_1, x_2, \ldots, x_n)$,
  \[
    \Pr((X_1, X_2, \ldots, X_n) \in C) = \iint \cdots \int_C f(x_1, x_2, \ldots, x_n) \, dx_1 \, dx_2 \, \ldots \, dx_n
  \]
\end{definition}

\begin{example}
  The multinomial distribution is a joint distribution.
\end{example}

\section{References}

\begin{enumerate}
  \item A First Course in Probability (10th edition) by Sheldon Ross
  \item \href{https://chrispiech.github.io/probabilityForComputerScientists/en/}{Stanford CS109 readings}
  \item \href{https://web.stanford.edu/class/archive/cs/cs109/cs109.1234/}{Stanford CS109 course resources}
\end{enumerate}

\end{document}
