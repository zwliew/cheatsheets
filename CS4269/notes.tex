\documentclass[11pt,usenames, dvipsnames]{article}


\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[scaled=.96,osf,sups]{XCharter}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{amsfonts}

% \usepackage{newproof}
% \usepackage{handout}

\usepackage{fancyhdr}
% \let\proof\relax
% \let\endproof\relax
\usepackage{amsmath,comment,amsthm}
\usepackage{cancel}

\usepackage{graphicx}
\usepackage[margin=1.5in]{geometry}
\usepackage[inline]{enumitem}
\usepackage{tikz}

\usepackage{proof}

% \usepackage[usenames, dvipsnames]{xcolor}


% \let\mathcal\undefined
% \DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\DeclareRobustCommand{\euscr}[1]{%
  \text{\usefont{U}{eus}{m}{n}#1}%
}

% \newcommand\hmmax{0}
% \newcommand\bmmax{0}

% \usepackage{calrsfs}
% \DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}

\input{../macros}

% \pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Do your customization here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\masunitnumber}{CS 4269/5469}
\newcommand{\examdate}{\today}
\newcommand{\academicyear}{2022-2023}
\newcommand{\semester}{II}
\newcommand{\coursename}{Fundamentals of Logic In Computer Science}
\newcommand{\numberofhours}{2}

%\DeclareMathOperator{\diam}{diam}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Don't touch anything from here till instructions
% to candidates
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{}
\rhead{}
\chead{{\bf NATIONAL UNIVERSITY OF SINGAPORE}}
\lfoot{}
\rfoot{}
\cfoot{}
\begin{document}
\setlength{\headsep}{5truemm}
\setlength{\headheight}{14.5truemm}
\setlength{\voffset}{-0.45truein}
\renewcommand{\headrulewidth}{0.0pt}

\begin{center}
{\Large \bf \masunitnumber -- \coursename}
\end{center}

\begin{center}
SEMESTER \semester, \academicyear
\end{center}

\begin{center}
\underline{Overall Notes}\\
\end{center}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering First-Order Logic}
\noindent

First-order Logic (FOL) is a formal language to describe and reason about \emph{predicates} rather than propositions. A predicate is a proposition that depends on the value of some variables. To do this, first-order logic builds upon propositional logic with functions, variables, and quantification.

\section*{\large \centering Motivation behind FOL}
\noindent

First-order logic grew out of the desire to study the foundations of mathematics in number theory and set theory. To illustrate the need for FOL, recall that we can represent the following statements as propositions in propositional logic:
\begin{align*}
  s &= \text{"Greeks are humans."}
  \\
  r &= \text{"Humans are mortals."}
  \\
  p &= \text{"Greeks are mortals."}
\end{align*}

However, the limited expressiveness of propositional logic prevents us from reasoning about the elements in a universe. Thus, propositional logic cannot properly encode the following statements:
\begin{align*}
  s_1 &= \text{"If a person is a Greek, then he is a human." or } \forall x \ldotp G(x) \rightarrow H(x).
  \\
  s_2 &= \text{"There is a Greek." or } \exists x \ldotp G(x).
  \\
  s_3 &= \text{"There is a human." or } \exists x \ldotp H(x).
\end{align*}

Here, $G$ and $H$ are \emph{predicates} where $G(x)$ means $x$ is a Greek and $H(x)$ means $x$ is a human. 

\subsection*{\large \centering Syntax of FOL}
\noindent

We start by defining the syntax of first-order logic. First-order logic formulae are defined over a signature that identifies non-logical symbols, namely, the predicates, constants, and functions that can be used in formulae.

\begin{definition}[Signature]
  A \emph{signature} or \emph{vocabulary} is $\Sigma = (\conn, \func, \reln)$ where
  \begin{enumerate}
    \item $\conn = \{c_1, c_2, \ldots \}$ is a set of \emph{constant symbols}.
    \item $\func = \{f_1, f_2, \ldots\}$ is a set of \emph{function symbols}. Each $f \in \func$ has an associated arity, denoted $\arity{f} \in \mathbb{N}_{\ge 0}$.
    \item $\reln = \{R_1, R_2, \ldots\}$ is a set of \emph{relation symbols}. Each $R \in \reln$ has an associated arity, denoted $\arity{R} \in \mathbb{N}_{> 0}$.
  \end{enumerate}
\end{definition}

Besides the signature, we also need a set $\vars = \{x_1, x_2, \ldots\}$ of variables. We typically consider signatures $\Sigma$ and variables $\vars$ that are countable. Lastly, we also inherit the propositional connectives $\vee$ and $\neg$.

Now, we can define the set of \emph{terms} in first-order logic.

\begin{definition}[Terms]
  A \emph{terms} over a signature $\Sigma = (\conn, \func, \reln)$ and variables $\vars$ is given by the following BNF grammar:
  $$
  t := x \mid c \mid f(t, t, \ldots, t)
  $$
  where $x \in \vars$, $c \in \conn$, and $f \in \func$. The number of terms in a function $f$ is determined by $\arity{f}$.
\end{definition}

Having defined terms, we can use them to define well-formed formulae (wff) or formulae for short.

\begin{definition}[Formulae]
  A \emph{well-formed formula (wff)} over a signature $\Sigma = (\conn, \func, \reln)$ and variables $\vars$ is given by the following BNF grammar:
  $$
  \varphi := t = t \mid R(t, t, \ldots, t) \mid (\neg \varphi) \mid (\varphi \vee \varphi) \mid (\exists x \ldotp \varphi)
  $$
  Here, $t$ is a term, $x \in \vars$ is a variable, and $R \in \reln$ is a relation. The number of terms in a relation $R$ is determined by $\arity{R}$.
\end{definition}

We will additionally use the derived operators "$\wedge$", "$\rightarrow$" and the derived \emph{universal} quantifier "$\forall$" which are obtained as follows:
\begin{enumerate}
  \item $\varphi_1 \wedge \varphi_2 \equiv \neg((\neg \varphi_1) \vee (\neg \varphi_2))$
  \item $\varphi_1 \rightarrow \varphi_2 \equiv (\neg \varphi_1) \vee \varphi_2$
  \item $\forall x \ldotp \varphi \equiv \neg(\exists x \ldotp (\neg \varphi))$
\end{enumerate}

From here on, when we say \emph{formulae}, we really mean \emph{well-formed formulae} in first-order logic. Furthermore, we will omit parantheses where the order of operations is clear. For example, we will write $\neg \varphi \vee \varphi_2$ instead of $(\neg \varphi) \vee \varphi_2$.

A formula $\varphi$ is an \emph{atomic formula} if it does not have any logical operators; that is, it is of the form $t_1 = t_2$ or $R(t_1, t_2, \ldots, t_k)$. Lastly, a \emph{literal} is a formula that is either atomic or the negation of an atomic formula.

\begin{example}
  Consider the signature $\Sigma = (\{c\}, \{f^1\}, \{<^2\})$ where $f$ has arity $1$ and $<$ has arity $2$. Furthermore, suppose that $\vars = \{x, y\}$. Then, the following are formulae:
  \begin{enumerate}
    \item $\exists x \ldotp \forall y \ldotp <(x, y) \vee x = y$
    \item $\forall x \ldotp \forall y \ldotp x = y$
    \item $\forall x \ldotp x = f(c)$
    \item $x = f(c)$
  \end{enumerate}
\end{example}

Note that in formula $4$, the variable $x$ is not quantified. In this case, $x$ is known as a \emph{free variable}. On the other hand, all of the variables in formula $1$ to $3$ are quantified and thus, they are \emph{bound variables}. Formulae with no free variables are known as \emph{sentences}.

\section*{\large \centering Semantics of FOL}
\noindent

The semantics of formulae in any logic is defined with respect to a \emph{model}. In propositional logic, models were truth assignments to the propositions. For first-order logic, models are known as \emph{structures} that help identify the interpretation of symbols in the signature.

\begin{definition}[Structure]
  Given a signature $\Sigma = (\conn, \func, \reln)$, a \emph{structure} $\euscr{A}$ over $\Sigma$ is a tuple $(U, I)$ where:
  \begin{itemize}
    \item $U$ is a non-empty set known as the \emph{universe/domain} of the structure,
    \item For each constant symbol $c \in \conn$, $I(c) \in U$ is its interpretation,
    \item For each function symbol $f \in \func$, $I(f) \colon U^{\arity{f}} \rightarrow U$ is its interpretation, and
    \item For each relation symbol $R \in \reln$, $I(R) \subseteq U^{\arity{R}}$ is its interpretation.
  \end{itemize}

  The structure is \emph{finite} if the universe $U$ is finite.
\end{definition}

\begin{example}
  Consider the signature $\Sigma = (\{0\}, \{S^1\}, \{<^2\})$. Then, we have a $\Sigma$-structure $\euscr{A} = (U, I)$ given by:
  \begin{itemize}
    \item $U = \mathbb{N}$,
    \item $I(0) = 0$,
    \item $I(s)(x) = x + 1$ for all $x \in U$, and
    \item $I(<) = \{(a, b) \mid a < b \text{ in the usual sense}\}$.
  \end{itemize}

  Now, consider the following formulae:
  \begin{enumerate}
    \item $\varphi_1 \equiv \exists x \ldotp <(0, x)$
    \item $\varphi_2 \equiv <(0, x)$
  \end{enumerate}

  As intuition leads, $\varphi_1$ is always true in $\euscr{A}$. On the other hand, the truthiness of $\varphi_2$ depends on the \emph{assignment} of $x$. This leads to following definition of assignments.
\end{example}

\begin{definition}[Assignment]
  For a $\Sigma$-structure $\euscr{A}$, an \emph{assignment} over $\euscr{A}$ is a function $\alpha \colon \vars \to U$ that assigns every variable $x \in \vars$ a value $\alpha(x) \in U$.
\end{definition}

We can then extend our choice of $\Sigma$-structure and assignment to a \emph{valuation} function over the set of $\Sigma$-terms. A valuation gives an interpretation to the terms in the language.

\begin{definition}[Valuation]
  For a $\Sigma$-structure $\euscr{A}$ and an assignment $\alpha$ over $\euscr{A}$, a \emph{valuation} $\textsf{val}_{\euscr{A}, \alpha} \colon \textsf{Terms} \to U$ is defined inductively as follows:
  \begin{itemize}
    \item For any variable $x \in \vars$, $\mathsf{val}_{\euscr{A}, \alpha}(x) = \alpha(x)$
    \item For any constant $c \in \conn$, $\mathsf{val}_{\euscr{A}, \alpha}(c) = I(c)$, and
    \item For any function $f^k \in \func$ where $k = \arity{f}$, we have
    $$
    \mathsf{val}_{\euscr{A}, \alpha}(f(t_1, t_2, \ldots, t_k)) = I(f)(\mathsf{val}_{\euscr{A}, \alpha}(t_1), \mathsf{val}_{\euscr{A}, \alpha}(t_2), \ldots, \mathsf{val}_{\euscr{A}, \alpha}(t_k))$$
  \end{itemize}
\end{definition}

Before we finally define the entailment rules for formulae, we have to first define a new shorthand notation for \emph{reassignment}. This helps us define the semantics of quantifiers.

\begin{definition}[Reassignment]
  For an assignment $\alpha \colon \vars \to U$ over a $\Sigma$-structure $\euscr{A} = (U, I)$, $\alpha[x \mapsto e]$ is the assignment
  $$
  \alpha[x \mapsto u](y) = \begin{cases}
    \alpha(y) & \text{if } y \ne x \\
    u & \text{otherwise.}
  \end{cases}
  $$
\end{definition}

Finally, we can define the semantics of FOL formulae.

\begin{definition}[Satisfaction]
  Given a $\Sigma$-structure $\euscr{A}$ and an assignment $\alpha$, the \emph{satisfaction} relation is a ternary relation $\models$. We write $\euscr{A}, \alpha \models \varphi$ to mean that "$\varphi$ holds in $\euscr{A}$ under assignment $\alpha$. We also write $\euscr{A}, \alpha \not\models \varphi$ to mean that $\euscr{A}, \alpha \models \varphi$ does not hold.
  
  The satisfaction relation $\models$ is inductively defined as follows:
  \begin{itemize}
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models t_1 = t_2$} iff $\mathsf{val}_{\euscr{A}, \alpha}(t_1) = \mathsf{val}_{\euscr{A}, \alpha}(t_2)$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models R(t_1, \ldots, t_k)$} iff $(\mathsf{val}_{\euscr{A}, \alpha}(t_1), \ldots, \mathsf{val}_{\euscr{A}, \alpha}(t_k)) \in I(R)$ where $k = \arity{R}$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models \neg \varphi$} iff $\euscr{A}, \alpha \not\models \varphi$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models \varphi \lor \psi$} iff $\euscr{A}, \alpha \models \varphi$ or $\euscr{A}, \alpha \models \psi$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models \exists x \ldotp \varphi$} iff there exists $u \in U$ such that $\euscr{A}, \alpha[x \mapsto u] \models \varphi$
  \end{itemize}
\end{definition}

We will now more formally define \emph{bound} and \emph{free} variables in a formula. We start by defining the \emph{scope} of a quantifier.

\begin{definition}[Scope]
  Given a formula $\varphi = \exists x \ldotp \psi$, $\psi$ is said to be the \emph{scope} of the quantifier $\exists x$.
\end{definition}

\begin{definition}[Bound and Free Variables]
  Every occurrence of the variable $x$ in $\varphi = \exists x \ldotp \psi$ is called a \emph{bound occurrence} of $x$ in $\psi$. Any occurrence of $x$ which is not bound is called a \emph{free occurrence} of $x$ in $\psi$.

  The free variables in $\varphi$ is denoted by $\mathsf{FVar}(\varphi)$. We go with the convention that a free variable of $\varphi$ must occur in $\varphi$ and thus $|\mathsf{FVar}(\varphi)|$ is bounded by the size of the formula $\varphi$.

  The set of bound and free occurrences of a variable $x \in \vars$ in a formula can be defined formally using induction on the structure of the formula, but is skipped here.
\end{definition}

Notice that a variable may occur both bound and free. For example, in the formula $\varphi_1 \equiv R(\mathbf{x}) \to \forall x \ldotp T(\underline{x}, f(\underline{x}))$, the bolded variable is free but the underlined variables are bound. We can \emph{rename} bound variables such that (a) bound variables are disjoint from free variables and (b) two bound occurrences of a variable refer to the same quantifier.

\begin{claim}
  For every formula $\varphi$, there is an equivalent formula $\psi$ such that the bound and free variables of $\psi$ are disjoint and every bound variable is in the scope of a unique quantifier.
\end{claim}

\begin{example}
  The formula $\varphi_2 \equiv R(\mathbf{x}) \to \forall y \ldotp T(\underline{y}, f(\underline{y}))$ is equivalent to $\varphi_1$.
\end{example}

\begin{definition}[Sentence]
  A \emph{sentence} is a formula $\varphi$ that has no free variables (i.e. $\mathsf{FVar}(\varphi) = \emptyset$).
\end{definition}

An analogous notion of the Relevance Lemma for FOL is the observation that the satisfaction of a formula depends only on the values that $\alpha$ assigns to the free variables of $\varphi$; the values assigned to bound variables are irrelevant. Before we prove the Relenvace Lemma for FOL, we first prove a related result on terms.

\begin{lemma}[Relevance Lemma on Terms]
  Let $t$ be a term and let $\struct = (U, I)$ be a structure. If assignments $\alpha_1$ and $\alpha_2$ are such that $\alpha_1(x) = \alpha_2(x)$ for each variable $x$ occurring in $t$, then $\val{\struct, \alpha_1}{t} = \val{\struct, \alpha_2}{t}$.
\end{lemma}

\begin{proof}[Proof]
  We will prove by induction on the structure of $t$ that if $\alpha_1(x) = \alpha_2(x)$ for each variable $x$ occurring in $t$, then $\val{\struct, \alpha_1}{t} = \val{\struct, \alpha_2}{t}$.

  \begin{description}
    \item[Base Case 1.] For any constant symbol $c \in \conn$, $\val{\struct, \alpha_1}{c} = I(c) = \val{\struct, \alpha_2}{c}$ by definition.

    \item[Base Case 2.] For any variable $x \in \vars$, $\val{\struct, \alpha_1}{x} = \alpha_1(x) = \alpha_2(x) = \val{\struct, \alpha_2}{x}$ by the assumption. 

    \item[Inductive Step.] Assume that the inductive hypothesis holds for some terms $t_1, \ldots t_k$ for some $k \in \nats$. Consider any function symbol $f^k \in \func$ where $k = \arity{f}$. Then, we have:
    \begin{align*}
    \val{\struct, \alpha_1}{f(t_1, \ldots, t_k)} &= I(f)(\val{\struct, \alpha_1}{t_1}, \ldots, \val{\struct, \alpha_1}{t_k}) &&\text{(by definition)}
    \\
    &= I(f)(\val{\struct, \alpha_2}{t_1}, \ldots, \val{\struct, \alpha_2}{t_k}) &&\text{(by inductive hypothesis)}
    \\
    &= \val{\struct, \alpha_2}{f(t_1, \ldots, t_k)} &&\text{(by definition)}
    \end{align*}
  \end{description}
  Thus, the inductive hypothesis holds for all terms $t$.
\end{proof}

Now, we can use the previous result to prove the Relevance Lemma on FOL formulae.

\begin{lemma}[Relevance Lemma on Formulae]
  Let $\varphi$ be a FOL formula and let $\struct$ be a structure. If assignments $\alpha_1$ and $\alpha_2$ are such that $\alpha_1(x) = \alpha_2(x)$ for every $x \in \fvar{\varphi}$, then $\struct, \alpha_1 \models \varphi$ iff $\struct, \alpha_2 \models \varphi$.
\end{lemma}

\begin{proof}
  Without loss of generality, suppose that $\struct, \alpha_1 \models \varphi$. We want to show that $\struct, \alpha_2 \models \varphi$. Once we have shown this, then by swapping $\alpha_1$ and $\alpha_2$ in our argument, we will have also proven that if $\struct, \alpha_2 \models \varphi$ then $\struct, \alpha_1 \models \varphi$.

  So, to proceed, we will prove by induction on the structure of $\varphi$ that if $\alpha_1(x) = \alpha_2(x)$ for every $x \in \fvar{\varphi}$, and $\struct, \alpha_1 \models \varphi$, then $\struct, \alpha_2 \models \varphi$.

  \begin{description}
    \item[Base Case 1.] For any atomic formula of the form $\varphi \equiv t_1 = t_2$ for some terms $t_1$ and $t_2$, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \val{\struct, \alpha_1}{t_1} &= \val{\struct, \alpha_1}{t_2} &&\text{(since $\struct, \alpha_1 \models \varphi$)}
      \\
      \val{\struct, \alpha_2}{t_1} &= \val{\struct, \alpha_2}{t_2} &&\text{(by the previous lemma)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    \item[Base Case 2.] For any atomic formula of the form $\varphi \equiv R(t_1, \ldots, t_k)$ for some terms $t_1, \ldots, t_k$ where $k = \arity{R}$, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      (\val{\struct, \alpha_1}{t_1}, \ldots, \val{\struct, \alpha_1}{t_k}) &\in I(R) &&\text{(by definition)}
      \\
      (\val{\struct, \alpha_2}{t_1}, \ldots, \val{\struct, \alpha_2}{t_k}) &\in I(R) &&\text{(by the previous lemma)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    \item[Inductive Step 1.] Assume that the inductive hypothesis holds for some formula $\psi$. Consider the formula $\varphi \equiv \neg \psi$. Then, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \struct, \alpha_1 &\not\models \psi &&\text{(by definition)}
      \\
      \struct, \alpha_2 &\not\models \psi &&\text{(by the inductive hypothesis)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    \item[Inductive Step 2.] Assume that the inductive hypothesis holds for some formulae $\psi_1$ and $\psi_2$. Consider the formula $\varphi \equiv \psi_1 \lor \psi_2$. Then, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \struct, \alpha_1 &\models \psi_1 \lor \psi_2 &&\text{(by definition)}
      \\
      \struct, \alpha_1 &\models \psi_1 \text{ or } \struct, \alpha_1 \models \psi_2 &&\text{(by definition)}
      \\
      \struct, \alpha_2 &\models \psi_1 \text{ or } \struct, \alpha_2 \models \psi_2 &&\text{(by the inductive hypothesis)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    \item[Inductive Step 3.] Assume that the inductive hypothesis holds for some formula $\psi$. Consider the formula $\varphi \equiv \exists x \ldotp \psi$. Then, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \struct, \alpha_1 &\models \exists x \ldotp \psi &&\text{(by definition)}
      \\
      \exists u \in U \ldotp \ \struct, \alpha_1[x \to u] &\models \psi &&\text{(by definition)}
      \\
      \exists u \in U \ldotp \ \struct, \alpha_2[x \to u] &\models \psi &&\text{(by the inductive hypothesis)}
      \\
      \struct, \alpha_2 &\models \exists x \ldotp \varphi &&\text{(by definition)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}
  \end{description}

  Hence, we have proven that if $\struct, \alpha_1 \models \varphi$, then $\struct, \alpha_2 \models \varphi$. Since our argument does not assume any additional property about $\alpha_1$ or $\alpha_2$, by swapping $\alpha_1$ and $\alpha_2$ in our argument, we can conclude that if $\struct, \alpha_2 \models \varphi$, then $\struct, \alpha_1 \models \varphi$.

  Therefore, we have proven that, assuming that $\alpha_1(x) = \alpha_2(x)$ for every $x \in \fvar{\varphi}$, then $\struct, \alpha_1 \models \varphi$ iff $\struct, \alpha_2 \models \varphi$.
\end{proof}

It follows from the Relevance Lemma that if $\varphi$ is a sentence, then all variable assignments are equivalent with respect to satisfiability. Hence, for any sentence $\varphi$, we simply write $\struct \models \varphi$ whenever $\struct, \alpha \models \varphi$ for some assignment $\alpha$.

\begin{corollary}
  For any sentence $\varphi$ and assignments $\alpha_1$ and $\alpha_2$, $\struct, \alpha_1 \models \varphi$ iff $\struct, \alpha_2 \models \varphi$.
\end{corollary}

\section*{\large \centering Satisfiability and Validity of FOL Formulae}
\noindent

We can define the satisfiability and validity of FOL formulae similar to the way we defined them for propositional logic.

\begin{definition}[Satisfiability]
  A FOL formula $\varphi$ over signature $\Sigma$ is \emph{satisfiable} if there is some structure $\struct$ and assignment $\alpha$ such that $\struct, \alpha \models \varphi$. Otherwise, $\varphi$ is \emph{unsatisfiable}.
\end{definition}

\begin{definition}[Satisfiability of a Set]
  A set of FOL formulae $\Gamma$ is \emph{satisfiable} if there is a structure $\struct$ and assignment $\alpha$ such that $\struct, \alpha \models \varphi$ for every $\varphi \in \Gamma$. Equivalently, we write $\struct, \alpha \models \Gamma$.
\end{definition}

\begin{definition}[Validity]
  A FOL formula $\varphi$ is said to be \emph{valid} if for every structure $\struct$ and assignment $\alpha$, $\struct, \alpha \models \varphi$.
\end{definition}

\begin{definition}[Logical Consequence]
  A formula $\varphi$ is a \emph{logical consequence} of a set of formulae $\Gamma$ if for each structure $\struct$ and assignment $\alpha$, $\struct, \alpha \models \Gamma$ implies that $\struct, \alpha \models \varphi$.

  As a shorthand, when $\emptyset \models \varphi$, we write $\models \varphi$.
\end{definition}

\begin{definition}[Logical Equivalence]
  Two formulae $\varphi_1$ and $\varphi_2$ are \emph{logically equivalent} if for every structure $\struct$ and assignment $\alpha$, $\struct, \alpha \models \varphi_1$ iff $\struct, \alpha \models \varphi_2$.
\end{definition}

\begin{definition}[Equisatisfiability]
  Two formulae $\varphi_1$ and $\varphi_2$ are \emph{equisatisfiable} when $\varphi_1$ and $\varphi_2$ are both satisfiable or both unsatisfiable.
\end{definition}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Primer on Countability}
\noindent

\begin{definition}[Injection]
  A function $f \colon A \to B$ is said to be \emph{injective} if for all $a_1, a_2 \in A$, if $f(a_1) = f(a_2)$, then $a_1 = a_2$.
\end{definition}

We aim to represent cardinality of sets in terms of injections.

\begin{definition}[Countable set]
  A set $S$ is said to be \emph{countable} if there exists an injection $f \colon S \to \mathbb{N}$.
\end{definition}

Notice that if a set is countable, then one can assign indices which are natural numbers to the elements of the set such that no two elements of $S$ get the same index.

This is analogous to the idea of "enumerating" the elements of $S$.

\begin{claim}[Finite sets are countable]
  \claimlabel{finite-countable}
  A finite set is countable.
\end{claim}

\begin{proof}[Proof sketch]
  Let $e$ be an arbitary enumeration of $S$. Consider the function $f \colon S \to \nats$ defined by $f(a) = i$ such that $i \in \nats$ is the index of $a$ in $e$; that is, $f(a) = e_i$. We can show that this is an injection
\end{proof}

To formally prove \claimref{finite-countable}, we will have to come up with an injective function from any finite set to the natural numbers. We will omit the proof here.

A more crucial observation is the following theorem.

\begin{theorem}
  \claimlabel{uncountable-exist}
  There are sets that are not countable; that is, they are \emph{uncountable}.
\end{theorem}

\begin{example}
  The set of real numbers $\reals$ and the powerset of the natural numbers $\powerset{\nats}$ are uncountable.
\end{example}

Next, we will state a straightforward claim.

\begin{claim}
  The set of natural numbers $\nats$ is countable.
\end{claim}

\begin{proof}[Proof sketch]
  Consider the function $f \colon \nats \to \nats$ defined by $f(n) = n$. We can show that this is an injection.
\end{proof}

Another simple observation is the following.

\begin{claim}
  \claimlabel{subset-countable}
  Let $S$ be a set and $S' \subseteq S$. If $S$ is countable, then $S'$ is countable.
\end{claim}

\begin{proof}[Proof sketch]
  Consider an injection $f \colon S \to \nats$. We can extend $f$ to an injection $g \colon S' \to \nats$ by defining $g(s) = f(s)$ for all $s \in S'$.
\end{proof}

\begin{example}
  Some other examples of countable sets are: the set of even numbers, odd numbers, and prime numbers. These are all subsets of the natural numbers.
\end{example}

Notice that while there is an injection from the set of even numbers to the set of naturals, there is also an injection from the set of naturals to the set of even numbers. We will formalise this notion in the following claim.

\begin{claim}
  If $A$ and $B$ are countably infinite sets, then there is a bijection between $A$ and $B$. Thus, $|A| = |B|$.
\end{claim}

An even more important result is the following claim:

\begin{claim}
  \claimlabel{cartesian-countable}
  Let $A$ and $B$ be countably infinite sets. Then, $A \times B = \{(a, b) \mid a \in A, b \in B\}$ is countable.
\end{claim}

The idea of a proof for \claimref{cartesian-countable} is to draw a grid and list the elements of $A$ along the rows and the elements of $B$ along the columns. Numbering the elements along the diagonals of the grid, we can then define an injection from $A \times B$ to $\nats$.

\claimref{cartesian-countable} yields the following corollaries:

\begin{corollary}
  \corlabel{cartesian-nats-countable}
  $\nats \times \nats$ is countable.
\end{corollary}

\begin{corollary}
  $\rationals$ is countable.
\end{corollary}

\begin{proof}[Proof sketch]
  Every rational $r \in \rationals$ is of the form $p/q$ where $p, q \in \nats$. Thus, $r$ can be represented as a pair $(p, q) \in \nats \times \nats$ and so $\rationals$ is isomorphic to a subset of $\nats \times \nats$. By \corref{cartesian-nats-countable} and \claimref{subset-countable}, $\rationals$ is countable.
\end{proof}

\begin{corollary}
  $\integers$ is countable.
\end{corollary}

\begin{proof}[Proof sketch]
  For every integer $i$, we can write it as either $(0, i)$ or $(-1, i)$. Thus, $\integers$ is isomorphic to a subset of $\nats \times \nats$. By \corref{cartesian-nats-countable} and \claimref{subset-countable}, $\rationals$ is countable.
\end{proof}

\begin{theorem}
  \thmlabel{uncountable-countable}
  There is a collection $\conn$ of countable sets such that $\conn$ is uncountable.
\end{theorem}

\begin{example}
  The powerset of the natural numbers $\powerset{\nats}$ is an uncountable collection of countable sets.
\end{example}

Next, we state a more important observation:

\begin{theorem}
  Let $\conn$ be a collection of countable sets such that $\conn$ is countable. Then, $\euscr{I} = \bigcup_{S \in \conn}{S}$ is countable.
\end{theorem}

\begin{proof}[Proof sketch]
  We can write each element $c \in \euscr{I}$ as $(i, j)$ where $i$ is the index of the set $S \in \conn$ and $j$ is the index of the element $x \in S$. Thus, $\euscr{I}$ is isomorphic to some subset of $\nats \times \nats$. By \corref{cartesian-nats-countable} and \claimref{subset-countable}, $\rationals$ is countable.
\end{proof}

Finally, we arrive at one of the most important theorems in countability.

\begin{theorem}[Uncountability of reals]
  \thmlabel{uncountable-reals}
  $\reals$ is uncountable.
\end{theorem}

\begin{proof}[Proof sketch]
  The proof is by Cantor's diagonal argument.
\end{proof}

\begin{theorem}[Uncountability of powerset of naturals]
  $\powerset{\nats}$ is uncountable.
\end{theorem}

\begin{proof}[Proof sketch]
  For any set $S \in \powerset{\nats}$, we can associate a unique real number $r$ to $S$ where $r = 0.\ldots$ where the $i^{th}$ decimal place is $1$ if the $i^{th}$ natural number is in $S$ and $0$ otherwise. Hence, $\powerset{\nats}$ is isomorphic to $\reals$. By \thmref{uncountable-reals}, $\powerset{\nats}$ is uncountable.
\end{proof}

Next, let's look at alphabets, strings, and languages.

\begin{theorem}
  Let $\Sigma$ be some countable alphabet (or set). $\Sigma^*$ is countable, where $\Sigma^*$ denotes the set of all finite strings over $\Sigma$.
\end{theorem}

\begin{theorem}
  If $S$ be a countable set, then the set $\mathscr{P}_{\text{fin}}{(S)}$ of finite subsets of $S$ is countable.
\end{theorem}

We have previously seen that every set of strings on a countable alphabet is countable. But what about the set of all languages?

\begin{question}
  Let $\Sigma$ be a countable alphabet. A language $L$ over $\Sigma$ is a subset of $\Sigma^*$. Is the collection of all languages over $\Sigma$ countable? What if $\Sigma$ is finite?
\end{question}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Motivations behind the Study of Logic}
\noindent

Professor Mathur's research interest is in the space of formal program verification. Formal program verification is the problem of determining if a program $P$ meets a specification $\Phi$; that is, $P \models \Phi$. This is a more complete way of verifying the correctness of programs compared to software engineering-style testing, but requires a background in mathematical logic.

Another motivation is in the realm of databases. We can model a query as a first-order logic formula, such as the formula $\phi \equiv \mathsf{Friends}(p_1, p_2)$ where the interpretation of $\mathsf{Friends}$ is $I(\mathsf{Friends}) = \{(p_1, p_2), (p_2, p_3), \ldots \}$ and the universe $U$ is the set of all persons.

Yet another motivation is in the study of complexity theory, which discusses whether polynomial-time algorithms exist to solve a problem and what the best algorithm to solve a problem is. If we were able to encode a computational problem as a logic formulae, then determining whether the problem is solvable in polynomial time could be equivalent to determining the satisfiability of the formula.

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Primer on Computability Theory}
\noindent

Computability Theory asks questions like "what is the complexity of solving a problem?", or more generally, "is the problem solvable?". We will study computability theory in the context of first-order logic (FOL), by modelling computational problems as formulae and determining if such formulae are satisfiable in polynomial time.

Here's an exercise: write a program $P$ that takes
\begin{enumerate}
  \item a program $Q$ as input,
  \item and an input $I$ to $Q$ as input.
\end{enumerate}
such that $P$ outputs "yes" if $Q$ on input $I$ prints "hello" as the first 5 characters, and "no" otherwise.

It turns out that it is impossible to write such a program.

\begin{claim}
  There is no program $P$ that takes as input (1) a program $Q$ and (2) an input $I$ to $Q$ such that $P$ outputs "yes" if $Q$ on input $I$ prints "hello" as the first 5 characters, and "no" otherwise.
\end{claim}

\begin{proof}
  Suppose, on the contrary, that there exists such a program $P$. Then, we can write a program $P'$ which takes in a program $Q$ and runs $P(Q, Q)$, then outputs "no" if $P(Q, Q)$ outputs "yes" else it outputs "no".

  Now, suppose we execute $P'(P')$. If it outputs "no", then $P(P', P')$ outputs "yes", which means that $P'(P')$ outputs "hello". This is a contradiction. Otherwise, if $P'(P')$ outputs "hello", then $P(P', P')$ outputs "no", which means that $P'(P')$ does not output "hello". This is also a contradiction.

  In either case, we arrive at a contradiction. Therefore, there is no such program $P$.
\end{proof}

The previous claim shows that certain computational problems that are unsolvable. Another well-known unsolvable problem is the Halting problem, which is the problem of determining whether a given program will terminate on a given input.

\section*{\large \centering Turing Machines}
\noindent

We use Turing machines as our main model of computation. Input that is fed into a Turing machine can be modelled as strings over an alphabet. We will define some terms and notation before proceeding.

\begin{definition}[Turing machine]
  A \emph{Turing machine} is a tuple $M = (Q, q_0, q\textsubscript{acc}, q\textsubscript{rej}, k, \delta, \Sigma)$ where
  \begin{enumerate}
    \item $Q$ is a finite set of control states
    \item $\Sigma$ is the alphabet of tape symbols
    \item $q_0 \in Q$ is the initial state
    \item $q\textsubscript{acc} \in Q$ is the accepting state
    \item $q\textsubscript{rej} \in Q$ is the rejecting state
  \end{enumerate}
\end{definition}

\begin{definition}[Configuration]
  A \emph{configuration} $C$ of a Turing machine $M$ is $C = (q, w\textsubscript{inp} \uparrow w'\textsubscript{inp}, w_{\text{WT}_1} \uparrow w'_{\text{WT}_1}, \ldots, w_{\text{WT}_k} \uparrow w'_{\text{WT}_k}, w\textsubscript{out} \uparrow w'\textsubscript{out})$ where $w_x$ are finite strings over $\Sigma$ representing the finite contents of the unbounded tape and $\uparrow$ refers to the pointer of the input tape, work tapes, and output tapes.
\end{definition}

\begin{definition}[Run/Computation]
  A \emph{run/computation} of Turing machine $M$ on input $w \in \Sigma^*$ is $\pi = c_0, c_1, \ldots, c_m$ such that $c_0$ is the initial configuration and, for each $i$, $c_{i+1}$ follows from $c_i$ using $\delta$.
\end{definition}

\begin{definition}[Accepting Run/Acceptance]
  A run $\pi$ is an \emph{accepting run} if there is a configuration $c$ in the run such that $c = (q\textsubscript{acc}, \ldots)$ and $q\textsubscript{rej}$ is not reached before $c$. The input $w$ is \emph{accepted} by $M$ iff the run $\pi$ on $M$ is an accepting run. Otherwise, $w$ is rejected.
\end{definition}

\begin{definition}[Language of a Turing Machine]
  The \emph{language} of a Turing machine $M$ is $L(M) = \{w \in \Sigma^* \mid w \text{ is accepted by $M$}\}$. A language $A \subseteq \Sigma^*$ is \emph{recognized/accepted} by $M$ if $A = L(M)$.
\end{definition}

\begin{definition}[Halting]
  A Turing machine $M$ \emph{halts} on input $w$ if there is a computation $\pi = c_0, c_1, \ldots, c_m$ such that $c_m = (q\textsubscript{acc}, \ldots)$ or $c_m = (q\textsubscript{rej}, \ldots)$. The run $\pi$ is called a \emph{halting run}.
\end{definition}

For a list of objects $O_1, O_2, \ldots, O_k$, we will use $\langle O_1, O_2, \ldots, O_k \rangle$ to denote their binary encoding. In particular, for a Turing machine $M$, $\langle M \rangle$ is its encoding as a binary string. We may then define the language $L\textsubscript{Halt} = \{\langle M, w \rangle \mid w \text{ on } M \text{ halts}\}$ Is there a Turing machine $H$ such that $L(H) = L\textsubscript{Halt}$?

The answer is yes: just simulate $M$ on $w$. This Turing machine is known as a \emph{universal Turing machine} since it can simulate the specification of an arbitrary Turing machine on arbitrary input.

\section*{\large \centering Recursive and Recursively Enumerable Languages}
\noindent

Recall that there are $3$ possible outcomes when a Turing machine $M$ runs on an input string $w$ --- $M$ may halt and accept $w$, $M$ may halt and reject $w$, or $M$ may not halt on $w$. Depending on how a Turing machine behaves,  we can define two different classes of problems solvable on a Turing machine.

\begin{definition}[Recursively Enumerable]
  A language $A$ is \emph{recursively enumerable/semi-decidable} if there is a Turing machine $M$ such that $A = L(M)$. We denote the set of all recursively enumerable languages as $\textsf{RE}$.
\end{definition}

\begin{example}
  $L\textsubscript{Halt}$ is recursively enumerable. That is, $L\textsubscript{Halt} \in \textsf{RE}$.
\end{example}

\begin{definition}[Recursive/Decidable]
  A language $A$ is \emph{recursive/decidable} if there is a Turing machine $M$ that halts on \emph{all} inputs and $A = L(M)$. We denote the set of all recursive languages as $\textsf{REC}$.
\end{definition}

As an example, consider the language $L\textsubscript{Sorted} = \{\langle l \rangle \mid l \text{ is a sorted list}\}$.

\begin{example}
  $L\textsubscript{Sorted} \in \textsf{REC}$ but $L\textsubscript{Halt} \not\in \textsf{REC}$.
\end{example}

Observe that a problem that is recursive is solvable by an algorithm that always halts. Thus, by definition, recursive languages are also recursively enumerable. This observation is equivalent to the following lemma:

\begin{lemma}[Recursive Implies Recursively Enumerable]
  \lemlabel{rec-implies-re}
  \textsf{REC} $\subseteq$ \textsf{RE}.
\end{lemma}

We also define the complement of a language.

\begin{definition}[Complement of a Language]
  The complement of a language $L$ is $\overline{L} = \Sigma^* \setminus L$.
\end{definition}

\begin{theorem}[Complement of Recursive is Recursive]
  If $L \in \textsf{REC}$, then $\overline{L} \in \textsf{REC}$.
\end{theorem}

\begin{proof}[Proof sketch]
  Take the Turing machine $M$ that accepts $L$ and let $M'$ be the Turing machine $M$ with the only difference being that the accepting and rejecting states are swapped. Then, $\overline{L} = L(M')$ and $M'$ halts on every input. Thus, $\overline{L} \in \textsf{REC}$.
\end{proof}

The following theorem is a useful way to prove that a problem is recursive.

\begin{theorem}
  \thmlabel{re-rec-link}
  $L$ is recursive iff $L$ and $\overline{L}$ are recursively enumerable. That is, $L \in \textsf{REC}$ iff $L \in \textsf{RE}$ and $\overline{L} \in \textsf{RE}$.
\end{theorem}

\begin{proof}[Proof sketch]
  The (easy) forward direction uses \lemref{rec-implies-re}.
  
  The reverse direction uses a technique called \emph{dovetailing}. Suppose that $L$ and $\overline{L}$ are recognized by $M$ and $\overline{M}$ respectively. Then, construct $M'$ by running both $M$ and $\overline{M}$ simultaneously on an input $w$, and accept if $M$ accepts or reject if $\overline{M}$ accepts. Since $w$ belongs to either $L$ or $\overline{L}$, $M'$ halts on all inputs and $L = L(M')$. Thus, $L \in \textsf{REC}$.
\end{proof}

Not every decision problem is recursively enumerable.

\begin{theorem}[Language outside \textsf{RE}]
  There is a language $L$ that is not recursively enumerable. That is, $L \not\in \textsf{RE}$.
\end{theorem}

\begin{proof}
  Since $L\textsubscript{Halt} \not\in \textsf{REC}$ but $L\textsubscript{Halt} \in \textsf{RE}$, by \thmref{re-rec-link}, $\overline{L\textsubscript{Halt}} \not\in \textsf{RE}$.

  Alternatively, note that there are uncountably many languages yet countably many Turing machines.
\end{proof}

Next, consider the language $K = \{\langle M \rangle \mid \langle M \rangle \in L(M)\}$. Using Cantor's diagonalization technique, we can establish that the complement $\overline{K}$ is not recursively enumerable.

\begin{theorem}
  The language $\overline{K} = \{\langle M \rangle \mid \langle M \rangle \not\in L(M)\}$ is not recursively enumerable. That is, $\overline{K} \notin \textsf{RE}$.
\end{theorem}

\begin{proof}
  Suppose to the contrary that $\overline{K} = L(M)$ for some Turing machine $M$. If $\langle M \rangle \in \overline{K}$, then $\langle M \rangle \not\in L(M) = \overline{K}$, which is a contradiction. Otherwise, if $\langle M \rangle \not\in \overline{K}$, then $\langle M \rangle \in L(M) = \overline{K}$, which is also a contradiction. Therefore, $\overline{K} \ne L(M)$ for any Turing machine $M$ and so $\overline{K} \notin \textsf{RE}$.
\end{proof}

\end{document}

