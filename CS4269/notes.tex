\documentclass[11pt,usenames, dvipsnames]{article}


\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[scaled=.96,osf,sups]{XCharter}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{amsfonts}

% \usepackage{newproof}
% \usepackage{handout}

\usepackage{fancyhdr}
% \let\proof\relax
% \let\endproof\relax
\usepackage{amsmath,comment,amsthm}
\usepackage{cancel}

\usepackage{graphicx}
\usepackage[margin=1.5in]{geometry}
\usepackage[inline]{enumitem}
\usepackage{tikz}

\usepackage{proof}

% \usepackage[usenames, dvipsnames]{xcolor}


% \let\mathcal\undefined
% \DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\DeclareRobustCommand{\euscr}[1]{%
  \text{\usefont{U}{eus}{m}{n}#1}%
}

% \newcommand\hmmax{0}
% \newcommand\bmmax{0}

% \usepackage{calrsfs}
% \DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}

\input{../macros}

% \pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Do your customization here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\masunitnumber}{CS 4269/5469}
\newcommand{\examdate}{\today}
\newcommand{\academicyear}{2022-2023}
\newcommand{\semester}{II}
\newcommand{\coursename}{Fundamentals of Logic In Computer Science}
\newcommand{\numberofhours}{2}

%\DeclareMathOperator{\diam}{diam}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Don't touch anything from here till instructions
% to candidates
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lhead{}
\rhead{}
\chead{{\bf NATIONAL UNIVERSITY OF SINGAPORE}}
\lfoot{}
\rfoot{}
\cfoot{}
\begin{document}
\setlength{\headsep}{5truemm}
\setlength{\headheight}{14.5truemm}
\setlength{\voffset}{-0.45truein}
\renewcommand{\headrulewidth}{0.0pt}

\begin{center}
{\Large \bf \masunitnumber -- \coursename}
\end{center}

\begin{center}
SEMESTER \semester, \academicyear
\end{center}

\begin{center}
\underline{Overall Notes}\\
\end{center}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Motivating the Study of Logic}
\noindent

Professor Mathur's research interest is in the space of formal program verification. Formal program verification is the problem of determining if a program $P$ meets a specification $\Phi$; that is, $P \models \Phi$. This is a more complete way of verifying the correctness of programs compared to software engineering-style testing, but requires a background in mathematical logic.

Another motivation is in the realm of databases. We can model a query as a first-order logic formula, such as the formula $\phi \equiv \mathsf{Friends}(p_1, p_2)$ where the interpretation of $\mathsf{Friends}$ is $I(\mathsf{Friends}) = \{(p_1, p_2), (p_2, p_3), \ldots \}$ and the universe $U$ is the set of all persons.

Yet another motivation is in the study of complexity theory, which discusses whether polynomial-time algorithms exist to solve a problem and what the best algorithm to solve a problem is. If we were able to encode a computational problem as a logic formulae, then determining whether the problem is solvable in polynomial time could be equivalent to determining the satisfiability of the formula.

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Proof Systems for Propositional Logic}
\noindent

A proof system provides a mechanical procedure for establishing a logical inference. They are a sequence of statements where each statement in the sequence is either a self-evident truth, or logically follows from previous observations. We will look at two different types of proof systems for porpositional logic: the \emph{Hilbert-style/Frege} proof system and the \emph{Resolution} proof system.

\section*{\large \centering Hilbert-style/Frege Proof System}
\noindent

The \emph{Hilbert-style} proof system provides a mechanical procedure for establishing new valid formulae.

\section*{\large \centering Resolution Proof System}
\noindent


\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering First-Order Logic}
\noindent

First-order Logic (FOL) is a formal language to describe and reason about \emph{predicates} rather than propositions. A predicate is a proposition that depends on the value of some variables. To do this, first-order logic builds upon propositional logic with functions, variables, and quantification.

\section*{\large \centering Motivation behind FOL}
\noindent

First-order logic grew out of the desire to study the foundations of mathematics in number theory and set theory. To illustrate the need for FOL, recall that we can represent the following statements as propositions in propositional logic:
\begin{align*}
  s &= \text{"Greeks are humans."}
  \\
  r &= \text{"Humans are mortals."}
  \\
  p &= \text{"Greeks are mortals."}
\end{align*}

However, the limited expressiveness of propositional logic prevents us from reasoning about the elements in a universe. Thus, propositional logic cannot properly encode the following statements:
\begin{align*}
  s_1 &= \text{"If a person is a Greek, then he is a human." or } \forall x \ldotp G(x) \rightarrow H(x).
  \\
  s_2 &= \text{"There is a Greek." or } \exists x \ldotp G(x).
  \\
  s_3 &= \text{"There is a human." or } \exists x \ldotp H(x).
\end{align*}

Here, $G$ and $H$ are \emph{predicates} where $G(x)$ means $x$ is a Greek and $H(x)$ means $x$ is a human.

\vspace{5truemm}
\hrule
\hrule

\subsection*{\large \centering Syntax of FOL}
\noindent

We start by defining the syntax of first-order logic. First-order logic formulae are defined over a signature that identifies non-logical symbols, namely, the predicates, constants, and functions that can be used in formulae.

\begin{definition}[Signature]
  A \emph{signature} or \emph{vocabulary} is $\Sigma = (\conn, \func, \reln)$ where
  \begin{enumerate}
    \item $\conn = \{c_1, c_2, \ldots \}$ is a set of \emph{constant symbols}.
    \item $\func = \{f_1, f_2, \ldots\}$ is a set of \emph{function symbols}. Each $f \in \func$ has an associated arity, denoted $\arity{f} \in \mathbb{N}_{\ge 0}$.
    \item $\reln = \{R_1, R_2, \ldots\}$ is a set of \emph{relation symbols}. Each $R \in \reln$ has an associated arity, denoted $\arity{R} \in \mathbb{N}_{> 0}$.
  \end{enumerate}
\end{definition}

Besides the signature, we also need a set $\vars = \{x_1, x_2, \ldots\}$ of variables. We typically consider signatures $\Sigma$ and variables $\vars$ that are countable. Lastly, we also inherit the propositional connectives $\vee$ and $\neg$.

Now, we can define the set of \emph{terms} in first-order logic.

\begin{definition}[Terms]
  A \emph{terms} over a signature $\Sigma = (\conn, \func, \reln)$ and variables $\vars$ is given by the following BNF grammar:
  $$
  t := x \mid c \mid f(t, t, \ldots, t)
  $$
  where $x \in \vars$, $c \in \conn$, and $f \in \func$. The number of terms in a function $f$ is determined by $\arity{f}$.
\end{definition}

Having defined terms, we can use them to define well-formed formulae (wff) or formulae for short.

\begin{definition}[Formulae]
  A \emph{well-formed formula (wff)} over a signature $\Sigma = (\conn, \func, \reln)$ and variables $\vars$ is given by the following BNF grammar:
  $$
  \varphi := t = t \mid R(t, t, \ldots, t) \mid (\neg \varphi) \mid (\varphi \vee \varphi) \mid (\exists x \ldotp \varphi)
  $$
  Here, $t$ is a term, $x \in \vars$ is a variable, and $R \in \reln$ is a relation. The number of terms in a relation $R$ is determined by $\arity{R}$.
\end{definition}

We will additionally use the derived operators "$\wedge$", "$\rightarrow$" and the derived \emph{universal} quantifier "$\forall$" which are obtained as follows:
\begin{enumerate}
  \item $\varphi_1 \wedge \varphi_2 \equiv \neg((\neg \varphi_1) \vee (\neg \varphi_2))$
  \item $\varphi_1 \rightarrow \varphi_2 \equiv (\neg \varphi_1) \vee \varphi_2$
  \item $\forall x \ldotp \varphi \equiv \neg(\exists x \ldotp (\neg \varphi))$
\end{enumerate}

From here on, when we say \emph{formulae}, we really mean \emph{well-formed formulae} in first-order logic. Furthermore, we will omit parantheses where the order of operations is clear. For example, we will write $\neg \varphi \vee \varphi_2$ instead of $(\neg \varphi) \vee \varphi_2$.

A formula $\varphi$ is an \emph{atomic formula} if it does not have any logical operators; that is, it is of the form $t_1 = t_2$ or $R(t_1, t_2, \ldots, t_k)$. Lastly, a \emph{literal} is a formula that is either atomic or the negation of an atomic formula.

\begin{example}
  Consider the signature $\Sigma = (\{c\}, \{f^1\}, \{<^2\})$ where $f$ has arity $1$ and $<$ has arity $2$. Furthermore, suppose that $\vars = \{x, y\}$. Then, the following are formulae:
  \begin{enumerate}
    \item $\exists x \ldotp \forall y \ldotp <(x, y) \vee x = y$
    \item $\forall x \ldotp \forall y \ldotp x = y$
    \item $\forall x \ldotp x = f(c)$
    \item $x = f(c)$
  \end{enumerate}
\end{example}

Note that in formula $4$, the variable $x$ is not quantified. In this case, $x$ is known as a \emph{free variable}. On the other hand, all of the variables in formula $1$ to $3$ are quantified and thus, they are \emph{bound variables}. Formulae with no free variables are known as \emph{sentences}.

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Semantics of FOL}
\noindent

The semantics of formulae in any logic is defined with respect to a \emph{model}. In propositional logic, models were truth assignments to the propositions. For first-order logic, models are known as \emph{structures} that help identify the interpretation of symbols in the signature.

\begin{definition}[Structure]
  Given a signature $\Sigma = (\conn, \func, \reln)$, a \emph{structure} $\euscr{A}$ over $\Sigma$ is a tuple $(U, I)$ where:
  \begin{itemize}
    \item $U$ is a non-empty set known as the \emph{universe/domain} of the structure,
    \item For each constant symbol $c \in \conn$, $I(c) \in U$ is its interpretation,
    \item For each function symbol $f \in \func$, $I(f) \colon U^{\arity{f}} \rightarrow U$ is its interpretation, and
    \item For each relation symbol $R \in \reln$, $I(R) \subseteq U^{\arity{R}}$ is its interpretation.
  \end{itemize}

  The structure is \emph{finite} if the universe $U$ is finite.
\end{definition}

\begin{example}
  Consider the signature $\Sigma = (\{0\}, \{S^1\}, \{<^2\})$. Then, we have a $\Sigma$-structure $\euscr{A} = (U, I)$ given by:
  \begin{itemize}
    \item $U = \mathbb{N}$,
    \item $I(0) = 0$,
    \item $I(s)(x) = x + 1$ for all $x \in U$, and
    \item $I(<) = \{(a, b) \mid a < b \text{ in the usual sense}\}$.
  \end{itemize}

  Now, consider the following formulae:
  \begin{enumerate}
    \item $\varphi_1 \equiv \exists x \ldotp <(0, x)$
    \item $\varphi_2 \equiv <(0, x)$
  \end{enumerate}

  As intuition leads, $\varphi_1$ is always true in $\euscr{A}$. On the other hand, the truthiness of $\varphi_2$ depends on the \emph{assignment} of $x$. This leads to following definition of assignments.
\end{example}

\begin{definition}[Assignment]
  For a $\Sigma$-structure $\euscr{A}$, an \emph{assignment} over $\euscr{A}$ is a function $\alpha \colon \vars \to U$ that assigns every variable $x \in \vars$ a value $\alpha(x) \in U$.
\end{definition}

We can then extend our choice of $\Sigma$-structure and assignment to a \emph{valuation} function over the set of $\Sigma$-terms. A valuation gives an interpretation to the terms in the language.

\begin{definition}[Valuation]
  For a $\Sigma$-structure $\euscr{A}$ and an assignment $\alpha$ over $\euscr{A}$, a \emph{valuation} $\textsf{val}_{\euscr{A}, \alpha} \colon \textsf{Terms} \to U$ is defined inductively as follows:
  \begin{itemize}
    \item For any variable $x \in \vars$, $\mathsf{val}_{\euscr{A}, \alpha}(x) = \alpha(x)$
    \item For any constant $c \in \conn$, $\mathsf{val}_{\euscr{A}, \alpha}(c) = I(c)$, and
    \item For any function $f^k \in \func$ where $k = \arity{f}$, we have
    $$
    \mathsf{val}_{\euscr{A}, \alpha}(f(t_1, t_2, \ldots, t_k)) = I(f)(\mathsf{val}_{\euscr{A}, \alpha}(t_1), \mathsf{val}_{\euscr{A}, \alpha}(t_2), \ldots, \mathsf{val}_{\euscr{A}, \alpha}(t_k))$$
  \end{itemize}
\end{definition}

Before we finally define the entailment rules for formulae, we have to first define a new shorthand notation for \emph{reassignment}. This helps us define the semantics of quantifiers.

\begin{definition}[Reassignment]
  For an assignment $\alpha \colon \vars \to U$ over a $\Sigma$-structure $\euscr{A} = (U, I)$, $\alpha[x \mapsto e]$ is the assignment
  $$
  \alpha[x \mapsto u](y) = \begin{cases}
    \alpha(y) & \text{if } y \ne x \\
    u & \text{otherwise.}
  \end{cases}
  $$
\end{definition}

Finally, we can define the semantics of FOL formulae.

\begin{definition}[Satisfaction]
  Given a $\Sigma$-structure $\euscr{A}$ and an assignment $\alpha$, the \emph{satisfaction} relation is a ternary relation $\models$. We write $\euscr{A}, \alpha \models \varphi$ to mean that "$\varphi$ holds in $\euscr{A}$ under assignment $\alpha$. We also write $\euscr{A}, \alpha \not\models \varphi$ to mean that $\euscr{A}, \alpha \models \varphi$ does not hold.
  
  The satisfaction relation $\models$ is inductively defined as follows:
  \begin{itemize}
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models t_1 = t_2$} iff $\mathsf{val}_{\euscr{A}, \alpha}(t_1) = \mathsf{val}_{\euscr{A}, \alpha}(t_2)$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models R(t_1, \ldots, t_k)$} iff $(\mathsf{val}_{\euscr{A}, \alpha}(t_1), \ldots, \mathsf{val}_{\euscr{A}, \alpha}(t_k)) \in I(R)$ where $k = \arity{R}$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models \neg \varphi$} iff $\euscr{A}, \alpha \not\models \varphi$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models \varphi \lor \psi$} iff $\euscr{A}, \alpha \models \varphi$ or $\euscr{A}, \alpha \models \psi$
    \item \makebox[4cm][l]{$\euscr{A}, \alpha \models \exists x \ldotp \varphi$} iff there exists $u \in U$ such that $\euscr{A}, \alpha[x \mapsto u] \models \varphi$
  \end{itemize}
\end{definition}

We will now more formally define \emph{bound} and \emph{free} variables in a formula. We start by defining the \emph{scope} of a quantifier.

\begin{definition}[Scope]
  Given a formula $\varphi = \exists x \ldotp \psi$, $\psi$ is said to be the \emph{scope} of the quantifier $\exists x$.
\end{definition}

\begin{definition}[Bound and Free Variables]
  Every occurrence of the variable $x$ in $\varphi = \exists x \ldotp \psi$ is called a \emph{bound occurrence} of $x$ in $\psi$. Any occurrence of $x$ which is not bound is called a \emph{free occurrence} of $x$ in $\psi$.

  The free variables in $\varphi$ is denoted by $\mathsf{FVar}(\varphi)$. We go with the convention that a free variable of $\varphi$ must occur in $\varphi$ and thus $|\mathsf{FVar}(\varphi)|$ is bounded by the size of the formula $\varphi$.

  The set of bound and free occurrences of a variable $x \in \vars$ in a formula can be defined formally using induction on the structure of the formula, but is skipped here.
\end{definition}

Notice that a variable may occur both bound and free. For example, in the formula $\varphi_1 \equiv R(\mathbf{x}) \to \forall x \ldotp T(\underline{x}, f(\underline{x}))$, the bolded variable is free but the underlined variables are bound. We can \emph{rename} bound variables such that (a) bound variables are disjoint from free variables and (b) two bound occurrences of a variable refer to the same quantifier.

\begin{claim}
  For every formula $\varphi$, there is an equivalent formula $\psi$ such that the bound and free variables of $\psi$ are disjoint and every bound variable is in the scope of a unique quantifier.
\end{claim}

\begin{example}
  The formula $\varphi_2 \equiv R(\mathbf{x}) \to \forall y \ldotp T(\underline{y}, f(\underline{y}))$ is equivalent to $\varphi_1$.
\end{example}

\begin{definition}[Sentence]
  A \emph{sentence} is a formula $\varphi$ that has no free variables (i.e. $\mathsf{FVar}(\varphi) = \emptyset$).
\end{definition}

An analogous notion of the Relevance Lemma for FOL is the observation that the satisfaction of a formula depends only on the values that $\alpha$ assigns to the free variables of $\varphi$; the values assigned to bound variables are irrelevant. Before we prove the Relenvace Lemma for FOL, we first prove a related result on terms.

\begin{lemma}[Relevance Lemma on Terms]
  Let $t$ be a term and let $\struct = (U, I)$ be a structure. If assignments $\alpha_1$ and $\alpha_2$ are such that $\alpha_1(x) = \alpha_2(x)$ for each variable $x$ occurring in $t$, then $\val{\struct, \alpha_1}{t} = \val{\struct, \alpha_2}{t}$.
\end{lemma}

\begin{proof}[Proof]
  We will prove by induction on the structure of $t$ that if $\alpha_1(x) = \alpha_2(x)$ for each variable $x$ occurring in $t$, then $\val{\struct, \alpha_1}{t} = \val{\struct, \alpha_2}{t}$.

  \begin{description}
    \item[Base Case 1.] For any constant symbol $c \in \conn$, $\val{\struct, \alpha_1}{c} = I(c) = \val{\struct, \alpha_2}{c}$ by definition.

    \item[Base Case 2.] For any variable $x \in \vars$, $\val{\struct, \alpha_1}{x} = \alpha_1(x) = \alpha_2(x) = \val{\struct, \alpha_2}{x}$ by the assumption. 

    \item[Inductive Step.] Assume that the inductive hypothesis holds for some terms $t_1, \ldots t_k$ for some $k \in \nats$. Consider any function symbol $f^k \in \func$ where $k = \arity{f}$. Then, we have:
    \begin{align*}
    \val{\struct, \alpha_1}{f(t_1, \ldots, t_k)} &= I(f)(\val{\struct, \alpha_1}{t_1}, \ldots, \val{\struct, \alpha_1}{t_k}) &&\text{(by definition)}
    \\
    &= I(f)(\val{\struct, \alpha_2}{t_1}, \ldots, \val{\struct, \alpha_2}{t_k}) &&\text{(by inductive hypothesis)}
    \\
    &= \val{\struct, \alpha_2}{f(t_1, \ldots, t_k)} &&\text{(by definition)}
    \end{align*}
  \end{description}
  Thus, the inductive hypothesis holds for all terms $t$.
\end{proof}

Now, we can use the previous result to prove the Relevance Lemma on FOL formulae.

\begin{lemma}[Relevance Lemma on Formulae]
  Let $\varphi$ be a FOL formula and let $\struct$ be a structure. If assignments $\alpha_1$ and $\alpha_2$ are such that $\alpha_1(x) = \alpha_2(x)$ for every $x \in \fvar{\varphi}$, then $\struct, \alpha_1 \models \varphi$ iff $\struct, \alpha_2 \models \varphi$.
\end{lemma}

\begin{proof}
  Without loss of generality, suppose that $\struct, \alpha_1 \models \varphi$. We want to show that $\struct, \alpha_2 \models \varphi$. Once we have shown this, then by swapping $\alpha_1$ and $\alpha_2$ in our argument, we will have also proven that if $\struct, \alpha_2 \models \varphi$ then $\struct, \alpha_1 \models \varphi$.

  So, to proceed, we will prove by induction on the structure of $\varphi$ that if $\alpha_1(x) = \alpha_2(x)$ for every $x \in \fvar{\varphi}$, and $\struct, \alpha_1 \models \varphi$, then $\struct, \alpha_2 \models \varphi$.

  \begin{description}
    \item[Base Case 1.] For any atomic formula of the form $\varphi \equiv t_1 = t_2$ for some terms $t_1$ and $t_2$, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \val{\struct, \alpha_1}{t_1} &= \val{\struct, \alpha_1}{t_2} &&\text{(since $\struct, \alpha_1 \models \varphi$)}
      \\
      \val{\struct, \alpha_2}{t_1} &= \val{\struct, \alpha_2}{t_2} &&\text{(by the previous lemma)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    \item[Base Case 2.] For any atomic formula of the form $\varphi \equiv R(t_1, \ldots, t_k)$ for some terms $t_1, \ldots, t_k$ where $k = \arity{R}$, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      (\val{\struct, \alpha_1}{t_1}, \ldots, \val{\struct, \alpha_1}{t_k}) &\in I(R) &&\text{(by definition)}
      \\
      (\val{\struct, \alpha_2}{t_1}, \ldots, \val{\struct, \alpha_2}{t_k}) &\in I(R) &&\text{(by the previous lemma)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    \item[Inductive Step 1.] Assume that the inductive hypothesis holds for some formula $\psi$. Consider the formula $\varphi \equiv \neg \psi$. Then, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \struct, \alpha_1 &\not\models \psi &&\text{(by definition)}
      \\
      \struct, \alpha_2 &\not\models \psi &&\text{(by the inductive hypothesis)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    Note that $\fvar{\varphi} = \fvar{\psi}$ by definition, so we can invoke the inductive hypothesis in the third step.

    \item[Inductive Step 2.] Assume that the inductive hypothesis holds for some formulae $\psi_1$ and $\psi_2$. Consider the formula $\varphi \equiv \psi_1 \lor \psi_2$. Then, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \struct, \alpha_1 &\models \psi_1 \lor \psi_2 &&\text{(by definition)}
      \\
      \struct, \alpha_1 &\models \psi_1 \text{ or } \struct, \alpha_1 \models \psi_2 &&\text{(by definition)}
      \\
      \struct, \alpha_2 &\models \psi_1 \text{ or } \struct, \alpha_2 \models \psi_2 &&\text{(by the inductive hypothesis)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    Note that any free variable $x$ in $\psi_1$ or $\psi_2$ is also free in $\varphi$. Hence, we can invoke the inductive hypothesis in the fourth step.

    \item[Inductive Step 3.] Assume that the inductive hypothesis holds for some formula $\psi$. Consider the formula $\varphi \equiv \exists x \ldotp \psi$. Then, we have:
    \begin{align*}
      \struct, \alpha_1 &\models \varphi &&\text{(by assumption)}
      \\
      \struct, \alpha_1 &\models \exists x \ldotp \psi &&\text{(by definition)}
      \\
      \exists u \in U \ldotp \ \struct, \alpha_1[x \to u] &\models \psi &&\text{(by definition)}
      \\
      \exists u \in U \ldotp \ \struct, \alpha_2[x \to u] &\models \psi &&\text{(by the inductive hypothesis)}
      \\
      \struct, \alpha_2 &\models \exists x \ldotp \psi &&\text{(by definition)}
      \\
      \struct, \alpha_2 &\models \varphi &&\text{(by definition)}
    \end{align*}

    Note that any free variable in $\psi$ is either free in $\exists x \ldotp \psi$ or bound to $x$. In either case, $\alpha_1[x \to u]$ and $\alpha_2[x \to u]$ agree on all free variables in $\psi$, and so we can invoke the inductive hypothesis in the fourth step.
  \end{description}

  Hence, we have proven that if $\struct, \alpha_1 \models \varphi$, then $\struct, \alpha_2 \models \varphi$. Since our argument does not assume any additional property about $\alpha_1$ or $\alpha_2$, by swapping $\alpha_1$ and $\alpha_2$ in our argument, we can conclude that if $\struct, \alpha_2 \models \varphi$, then $\struct, \alpha_1 \models \varphi$.

  Therefore, we have proven that, assuming that $\alpha_1(x) = \alpha_2(x)$ for every $x \in \fvar{\varphi}$, then $\struct, \alpha_1 \models \varphi$ iff $\struct, \alpha_2 \models \varphi$.
\end{proof}

\begin{corollary}
  For any sentence $\varphi$ and assignments $\alpha_1$ and $\alpha_2$, $\struct, \alpha_1 \models \varphi$ iff $\struct, \alpha_2 \models \varphi$.
\end{corollary}

It follows from the Relevance Lemma that if $\varphi$ is a sentence, then all variable assignments are equivalent with respect to satisfiability. Hence, for any sentence $\varphi$, we simply write $\struct \models \varphi$ whenever $\struct, \alpha \models \varphi$ for some assignment $\alpha$.

\section*{\large \centering Satisfiability and Validity of FOL Formulae}
\noindent

We can define the satisfiability and validity of FOL formulae similar to the way we defined them for propositional logic.

\begin{definition}[Satisfiability]
  A FOL formula $\varphi$ over signature $\Sigma$ is \emph{satisfiable} if there is some structure $\struct$ and assignment $\alpha$ such that $\struct, \alpha \models \varphi$. Otherwise, $\varphi$ is \emph{unsatisfiable}.
\end{definition}

\begin{definition}[Satisfiability of a Set]
  A set of FOL formulae $\Gamma$ is \emph{satisfiable} if there is a structure $\struct$ and assignment $\alpha$ such that $\struct, \alpha \models \varphi$ for every $\varphi \in \Gamma$. Equivalently, we write $\struct, \alpha \models \Gamma$.
\end{definition}

\begin{definition}[Validity]
  A FOL formula $\varphi$ is said to be \emph{valid} if for every structure $\struct$ and assignment $\alpha$, $\struct, \alpha \models \varphi$.
\end{definition}

\begin{definition}[Logical Consequence]
  A formula $\varphi$ is a \emph{logical consequence} of a set of formulae $\Gamma$ if for each structure $\struct$ and assignment $\alpha$, $\struct, \alpha \models \Gamma$ implies that $\struct, \alpha \models \varphi$.

  As a shorthand, when $\emptyset \models \varphi$, we write $\models \varphi$.
\end{definition}

\begin{definition}[Logical Equivalence]
  Two formulae $\varphi_1$ and $\varphi_2$ are \emph{logically equivalent} if for every structure $\struct$ and assignment $\alpha$, $\struct, \alpha \models \varphi_1$ iff $\struct, \alpha \models \varphi_2$.
\end{definition}

\begin{definition}[Equisatisfiability]
  Two formulae $\varphi_1$ and $\varphi_2$ are \emph{equisatisfiable} when $\varphi_1$ and $\varphi_2$ are both satisfiable or both unsatisfiable.
\end{definition}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Normalization and Skolemization}
\noindent

Similar to how propositional formulae in conjunctive normal form are easier to process, FOL formulae in \emph{prenex normal form} are also easier for automated theorem provers to process.

\begin{definition}[Prenex Normal Form]
  A formula is in \emph{prenex normal form} if it is of the form $\varphi \equiv Q_1x_1 \cdot Q_2x_2 \cdots Q_kx_k \cdot \psi$ where $Q_1, \ldots, Q_k \in \{ \forall, \exists \}$ are quantifiers, $x_1, \ldots, x_k \in \vars$ and $\psi$ is quantifier-free. $\psi$ is called the \emph{matrix} of the prenex normal formula formula $\varphi$.
\end{definition}

\begin{claim}
  Every FOL formula is logically equivalent to a formula in prenex normal form.
\end{claim}

One can go even further and eliminate existential quantifiers from a formula in prenex normal form via a process called \emph{Skolemization}. This results in an equisatisfiable formula.

\begin{definition}[Skolem Normal Form]
  A formula is in \emph{Skolem normal form} if it is in prenex normal form with only universal quantifiers (i.e. $\forall$).
\end{definition}

\begin{claim}[Skolemization]
  Every FOL formula is equisatisfiable with a formula in Skolem normal form. This equisatisfiable formula can be obtained via \emph{Skolemization}.
\end{claim}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Herbrand's Theorem for FOL}
\noindent

Herbrand's Theorem relates satisfiability of a set of FOL sentences to their propositional satisfiability. Before we state the theorem, we first state some new definitions.

\begin{definition}[Ground Term]
  A term over a signature $\Sigma$ is a \emph{ground term} if it does not contain any variables. The set of all ground terms over $\Sigma$ is denoted by $\terms_{\Sigma}$.
\end{definition}

\begin{definition}[Ground Formula]  
  A formula is a \emph{ground formula} if it contains no variables (and thus, also no quantifiers). The set of ground formulae over the signature $\Sigma$ is denoted by $\wff_{\Sigma}$.
\end{definition}
  
\begin{definition}[Ground Atom]
  A formula is a \emph{ground atom} if it is both a ground formula and an atomic formula. The set of ground atoms over the signature $\Sigma$ is denoted by $\mathsf{Atoms}_{\Sigma}$.
\end{definition}

\begin{definition}[Ground Instantiation]
  Let $\varphi \equiv \forall x_1 \forall x_2 \ldots \forall x_n \psi$ be a universally quantified FOL sentence over $\Sigma$ and $\vars$ in prenex normal form. A ground instantiation of $\varphi$ is a formula $\phi$ obtained by replacing, for every $1 \le i \le n$, each occurrence of $x_i$ in $\psi$ by some ground term $t_i$. That is, there is a mapping $T \colon \vars \to \terms_{\Sigma}$ such that $\phi \equiv \psi[x_1 \to T(x_1)]\ldots[x_n \to T(x_n)]$.
  
  We use $\mathsf{Ground}(\varphi)$ to denote the set $\{\psi[x_1 \to T(x_1)]\ldots[x_n \to T(x_n)] \mid T \colon \vars \to \terms_{\Sigma}\}$ of all ground instantiations of $\varphi$. For a set $\Gamma$ of universally quantified FOL sentences over $\Sigma$ and $\vars$, we denote $\mathsf{Ground}(\Gamma) = \bigcup\limits_{\varphi \in \Gamma}\mathsf{Ground}(\varphi)$.
\end{definition}

\begin{definition}[Propositional Satisfiability]
  A \emph{ground atomic valuation} over $\Sigma$ is a mapping $\tau \colon \mathsf{Atoms}_{\Sigma} \to \{ \truVal, \flsVal \}$. Such a valuation can be extended to ground formulae in the natural manner given by the relation $\grmodels$:
  \begin{align*}
    \tau &\grmodels \psi &&\text{iff $\tau(\psi) = \truVal$} &&\text{if } \psi \in \mathsf{Atoms}_{\Sigma} \\
    \tau &\grmodels (\neg \varphi) &&\text{iff $\tau \not\grmodels \varphi$} && \text{if $\varphi$ is ground} \\
    \tau &\grmodels (\varphi_1 \lor \varphi_2) &&\text{iff $\tau \grmodels \varphi_1$ or $\tau \grmodels \varphi_2$} && \text{if $\varphi_1, \varphi_2$ are ground}
  \end{align*}

  A set of ground FOL formulae $\Gamma$ is \emph{propositionally satisfiable} if there is a ground atomic valuation $\tau$ such that $\tau \grmodels \varphi$ for every $\varphi \in \Gamma$.
\end{definition}

We can begin discussing Herbrand's Theorem. As a start, we will only consider FOL formulae that are universally quantified sentences that do not have the symbol "=". We will refer to them as \emph{formulae without equality}.

\begin{theorem}[Herbrand's Theorem without Equality]
  Let $\Sigma$ be a signature with at least one constant. Let $\Gamma$ be a set of universally quantified FOL sentences without equality in prenex normal form. $\Gamma$ is satisfiable iff $\mathsf{Ground}(\Gamma)$ is propositionally satisfiable.
\end{theorem}

We shall prove this theorem in two directions. The first direction is easier, and the second direction is more difficult. We first prove the easier direction:

\begin{lemma}[Easier direction of Herbrand's Theorem without Equality]
  Let $\Sigma$ be a signature with at least one constant. Let $\Gamma$ be a set of universally quantified FOL sentences without equality in prenex normal form. If $\Gamma$ is satisfiable, then $\mathsf{Ground}(\Gamma)$ is propositionally satisfiable.
\end{lemma}

\begin{proof}
  Consider a structure $\struct$ such that $\struct \models \Gamma$. Let $\tau_\struct \colon \mathsf{Atoms}_\Sigma \to \{\truVal, \flsVal\}$ be the ground atomic valuation defined as follows:
  $$
  \tau_\struct(\varphi) = \begin{cases}
    \truVal &\text{if } \struct \models \varphi \\
    \flsVal &\text{otherwise}
  \end{cases}
  $$

  We will first prove the following property about $\tau_\struct$ that we will later use in our proof of the lemma: 
  \begin{claim}
    For any ground formula $\varphi$, $\struct \models \varphi$ iff $\tau_\struct \grmodels \varphi$.
  \end{claim}

  \begin{subproof}[Subproof.]
    We will prove, by structural induction on $\varphi$, that $\struct \models \varphi$ iff $\tau_\struct \grmodels \varphi$.

    \begin{description}
      \item[Base Case.] Consider a ground atom $\varphi$. If $\struct \models \varphi$, then $\tau_\struct(\varphi) = \truVal$ by definition, and so $\tau_\struct \grmodels \varphi$. Conversely, if $\struct \not\models \varphi$, then $\tau_\struct(\varphi) = \flsVal$ by definition, and so $\tau_\struct \not\grmodels \varphi$.

      \item[Inductive Case 1.] Assume the inductive hypothesis holds for some ground formula $\psi$. Consider the ground formula $\varphi \equiv (\neg \psi)$.
      
      If $\struct \models \varphi$, then $\struct \not\models \psi$ by definition. Then, by the inductive hypothesis, $\tau_\struct \not\grmodels \psi$ and so $\tau_\struct \grmodels \varphi$ by definition.
      
      Conversely, if $\struct \not\models \varphi$, then $\struct \models \psi$ by definition. Then, by the inductive hypothesis, $\tau_\struct \grmodels \psi$ and so $\tau_\struct \not\grmodels \varphi$ by definition.

      \item[Inductive Case 2.] Assume the inductive hypothesis holds for some ground formulae $\psi_1$ and $\psi_2$. Consider the ground formula $\varphi \equiv (\psi_1 \lor \psi_2)$.
      
      If $\struct \models \varphi$, then $\struct \models \psi_1$ or $\struct \models \psi_2$ by definition. Then, by the inductive hypothesis, $\tau_\struct \grmodels \psi_1$ or $\tau_\struct \grmodels \psi_2$ and so $\tau_\struct \grmodels \varphi$ by definition.

      Conversely, if $\struct \not\models \varphi$, then $\struct \not\models \psi_1$ and $\struct \not\models \psi_2$. Then, by the inductive hypothesis, $\tau_\struct \not\grmodels \psi_1$ and $\tau_\struct \not\grmodels \psi_2$ and so $\tau_\struct \not\grmodels \varphi$ by definition.
    \end{description}

    Hence, we have shown that $\struct \models \varphi$ iff $\tau_\struct \grmodels \varphi$.
  \end{subproof}

  Now, using the aforementioned claim, we will prove that $\mathsf{Ground}(\Gamma)$ is propositionally satisfiable.
  
  Consider any ground formula $\varphi \in \mathsf{Ground}(\Gamma)$. $\varphi$ must have been instantiated from a universally quantified sentence $\varphi' \in \Gamma$. Since $\struct \models \varphi'$, we must have $\struct \models \varphi$. Hence, by the aforementioned lemma, $\tau_\struct \grmodels \varphi$. Thus, $\mathsf{Ground}(\Gamma)$ is propositionally satisfiable.
\end{proof}

Now, we can progress towards proving the opposite direction. The proof of the opposite direction relies on the observation that a special simple structure, called the \emph{Herbrand structure}, satisfies $\Gamma$. We first define the Herbrand structure.

\begin{definition}[Herbrand Structure]
  Let $\Sigma$ be a signature with at least one constant symbol. A $\Sigma$-Herbrand-structure is a structure $\struct_\Sigma = (U_\Sigma, I_\Sigma)$ defined as follows:
  \begin{itemize}
    \item $U_\Sigma = \mathsf{Terms}_\Sigma$ is the set of all ground terms in $\Sigma$. It is also called the \emph{Herbrand universe}.
    \item $I_\Sigma$ assigns the unique "natural" interpretation to the constant and function symbols in $\Sigma$ (also called the \emph{Herbrand interpretation}). That is,
    \begin{itemize}
      \item For every constant symbol $c \in \conn$, $I_\Sigma(c) = c$.
      \item For every $k$-ary function symbol $f \in \func$ and for all ground terms $t_1, \ldots, t_k \in U_\Sigma$, $I_\Sigma(f)(t_1, \ldots, t_k) = f(t_1, \ldots, t_k)$.
    \end{itemize}
  \end{itemize}

  Thus, a Herbrand structure is built from syntax, with terms and function symbols being interpreted "as themselves".

  Observe that $I_\Sigma$ does not restrict what interpretations must be given to the relation symbols, and any meaning for symbols in $\reln$ can be picked as part of $I_\Sigma$.
\end{definition}

Now, we can prove the opposite direction of Herbrand's theorem without equality.

\begin{lemma}[Harder direction of Herbrand's theorem without Equality]
  Let $\Sigma$ be a signature with at least one constant symbol. Let $\Gamma$ be a set of universally quantified FOL sentences without equality in prenex normal form. If $\mathsf{Ground}(\Gamma)$ is propositionally satisfiable, then $\Gamma$ is satisfiable.
\end{lemma}

\begin{proof}
  Consider a ground atomic valuation $\tau$ such that $\tau \grmodels \mathsf{\psi}$ for every ground formula $\psi \in \mathsf{Ground}(\Gamma)$. Construct the Herbrand structure $\struct_\Sigma^\tau$ such that for every $k$-ary relation symbol $R \in \reln$, $I(R) = \{(t_1, \ldots, t_k) \mid t_1, \ldots, t_k \in \mathsf{Terms}_\Sigma \text{ and } \tau(R(t_1, \ldots, t_k)) = \truVal\}$.
  
  We will first prove properties about $\struct_\Sigma^\tau$ that we will later use in our proof of the lemma:
  \begin{claim}
    Let $\varphi \in \Gamma$ be a ground formula without equality. If $\tau \grmodels \varphi$, then $\struct_\Sigma^\tau \models \varphi$.
  \end{claim}

  \begin{subproof}[Subproof.]
    We will prove, by structural induction on $\varphi$, that if $\tau \grmodels \varphi$, then $\struct_\Sigma^\tau \models \varphi$.

    \begin{description}
      \item[Base Case.] Consider $\varphi \equiv R(t_1, \ldots, t_k)$ for some $k$-ary relation $R \in \reln$ and terms $t_1, \ldots, t_k$. Note that $\varphi$ is a ground atom and $t_1, \ldots, t_k$ are ground terms. If $\tau \grmodels \varphi$, then $\tau(\varphi) = \truVal$ and so $(t_1, \ldots, t_k) \in I(R)$. Hence, by definition, $\struct_\Sigma^\tau \models \varphi$.

      \item[Inductive Case 1.] Assume the inductive hypothesis for some ground formula $\psi$. Consider $\varphi \equiv (\neg \psi)$. Note that $\varphi$ is still a ground formula. If $\tau \grmodels \varphi$, this means that $\tau \not\grmodels \psi$. Then, by the inductive hypothesis, $\struct_\Sigma^\tau \not\models \psi$. Hence, by definition, $\struct_\Sigma^\tau \models \varphi$. 
      
      \item[Inductive Case 2.] Assume the inductive hypothesis for some ground formulae $\psi_1$ and $\psi_2$. Consider $\varphi \equiv (\psi_1 \lor \psi_2)$. Note that $\varphi$ is still a ground formula. If $\tau \grmodels \varphi$, then $\tau \grmodels \psi_1$ or $\tau \grmodels \psi_2$ by definition. By the inductive hypothesis, $\struct_\Sigma^\tau \models \psi_1$ or $\struct_\Sigma^\tau \models \psi_2$. Hence, by definition, $\struct_\Sigma^\tau \models \varphi$.
    \end{description}

    Thus, we have proven that, for every ground formula $\varphi$, $\struct_\Sigma^\tau \models \varphi$ if $\tau \grmodels \varphi$.
  \end{subproof}

  Next, we'll prove a stronger version of the claim that will be useful in our proof of the lemma.

  \begin{claim}
    Let $\varphi \equiv \forall x_1 \ldots \forall x_k \ldotp \psi$ in $\Gamma$ be a sentence without equality in prenex normal form. If $\tau$ propositionally satisfies $\mathsf{Ground}(\varphi)$, then $\struct_\Sigma^\tau \models \varphi$.
  \end{claim}

  \begin{subproof}[Subproof.]
    We will prove, by structural induction on the number of quantifiers $k$ in $\varphi$, that if $\tau$ propositionally satisfies $\mathsf{Ground}(\varphi)$, then $\struct_\Sigma^\tau \models \varphi$.

    \begin{description}
      \item[Base Case.] Consider the case $k = 0$. This means that $\varphi \equiv \psi$ where $\psi$ is a matrix. Since $\varphi$ is a ground formula, $\varphi \in \mathsf{Ground}(\varphi)$. If $\tau$ satisfies $\mathsf{Ground}(\varphi)$, $\tau \grmodels \varphi$. By the previous claim, we can conclude that $\struct_\Sigma^\tau \models \varphi$.
      
      \item[Inductive Case.] Assume the inductive hypothesis holds for some $k \in \nats$. Consider $\varphi \equiv \forall x_1 \ldots \forall x_{k+1} \ldotp \psi$ where $\psi$ is a matrix. If $\tau$ satisfies $\mathsf{Ground}(\varphi)$, then in particular $\tau$ satisfies $\mathsf{Ground}(\forall x_2 \ldots \forall x_{k+1} \ldotp \psi [x_1 \to t])$ for every ground term $t \in \terms_\Sigma$. By the inductive hypothesis, for every ground term $t \in \terms_\Sigma$, $\struct_\Sigma^\tau \models \forall x_2 \ldots \forall x_{k+1} \ldotp \psi [x_1 \to t]$. Equivalently, there is no $t \in \terms_\Sigma$ such that $\struct_\Sigma^\tau \not\models \forall x_2 \ldots \forall x_{k+1} \ldotp \psi [x_1 \to t]$. Hence, by definition, $\struct_\Sigma^\tau \models \varphi$.
    \end{description}

    Thus, we have proven that, for every sentence $\varphi$ in prenex normal form, $\struct_\Sigma^\tau \models \varphi$ if $\tau$ propositionally satisfies $\mathsf{Ground}(\varphi)$.
  \end{subproof}

  Now, we finally complete the proof of the lemma. Consider any sentence $\varphi$ in $\Gamma$. By the assumption and by definition of $\mathsf{Ground}(\Gamma)$, $\tau \grmodels \mathsf{Ground}(\varphi)$. Then, by the previous claim, $\struct_\Sigma^\tau \models \varphi$. Hence, $\struct_\Sigma^\tau \models \Gamma$.
\end{proof}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Implications of Herbrand's Theorem}
\noindent

The proof of Herbrand's Theorem highlights some interesting \emph{model-theoretic} observations. We go through them in this section.

First, the proof illustrates that whenever there is a model $\struct$ for a set $\Gamma$ of universally quantified FOL sentences without equality, there is a model which is not \emph{too large}. This is known as the \emph{Downward Löwenheim-Skolem Theorem}.

\begin{theorem}[Downward Löwenheim-Skolem Theorem]
  Let $\Sigma$ be a countable signature and let $\Gamma$ be a set of universally quantified FOL sentences without equality. If $\Gamma$ is satisfiable, then there is a countable structure $\struct$ such that $\struct \models \Gamma$.
\end{theorem}

\begin{proof}
  Since $\Gamma$ is satisfiable, $\mathsf{Ground}(\Gamma)$ is propositionally satisfiable. By the proof of Herbrand's Theorem, there is a Herbrand structure $\struct_\Sigma^\tau$ that satisfies $\Gamma$. We will show that $\struct_\Sigma^\tau$ is countable, which will then imply the Downward Löwenheim-Skolem Theorem.

  It suffices to show that $U_\Sigma = \terms_\Sigma$ is countable. By assumption, $\Sigma$ is countable and so for any natural number $k \in \nats$, the set of all ground terms of length $k$ is countable (since it is isomorphic to some subset of $\mathbb{N}^k$, which is countable). Then, $\terms_\Sigma = \bigcup\limits_{k \in \nats}\{\text{all ground terms of length } k\}$ is a countable union of countable sets and so, it is also countable. Hence, the Herbrand structure $\struct_\Sigma^\tau$ is countable.
\end{proof}

\section*{\large \centering Herbrand's Theorem with Equality}
\noindent

We have seen that in the absence of equality, a ground atomic valuation that makes all of $\mathsf{Ground}(\Gamma)$ propositionally satisfiable guarantees the existence of a structure that satisfies $\Gamma$. In the presence of equality, this is not true.

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Primer on Countability}
\noindent

\begin{definition}[Injection]
  A function $f \colon A \to B$ is said to be \emph{injective} if for all $a_1, a_2 \in A$, if $f(a_1) = f(a_2)$, then $a_1 = a_2$.
\end{definition}

We aim to represent cardinality of sets in terms of injections.

\begin{definition}[Countable set]
  A set $S$ is said to be \emph{countable} if there exists an injection $f \colon S \to \mathbb{N}$.
\end{definition}

Notice that if a set is countable, then one can assign indices which are natural numbers to the elements of the set such that no two elements of $S$ get the same index.

This is analogous to the idea of "enumerating" the elements of $S$.

\begin{claim}[Finite sets are countable]
  \claimlabel{finite-countable}
  A finite set is countable.
\end{claim}

\begin{proof}[Proof sketch]
  Let $e$ be an arbitary enumeration of $S$. Consider the function $f \colon S \to \nats$ defined by $f(a) = i$ such that $i \in \nats$ is the index of $a$ in $e$; that is, $f(a) = e_i$. We can show that this is an injection
\end{proof}

To formally prove \claimref{finite-countable}, we will have to come up with an injective function from any finite set to the natural numbers. We will omit the proof here.

A more crucial observation is the following theorem.

\begin{theorem}
  \claimlabel{uncountable-exist}
  There are sets that are not countable; that is, they are \emph{uncountable}.
\end{theorem}

\begin{example}
  The set of real numbers $\reals$ and the powerset of the natural numbers $\powerset{\nats}$ are uncountable.
\end{example}

Next, we will state a straightforward claim.

\begin{claim}
  The set of natural numbers $\nats$ is countable.
\end{claim}

\begin{proof}[Proof sketch]
  Consider the function $f \colon \nats \to \nats$ defined by $f(n) = n$. We can show that this is an injection.
\end{proof}

Another simple observation is the following.

\begin{claim}
  \claimlabel{subset-countable}
  Let $S$ be a set and $S' \subseteq S$. If $S$ is countable, then $S'$ is countable.
\end{claim}

\begin{proof}[Proof sketch]
  Consider an injection $f \colon S \to \nats$. We can extend $f$ to an injection $g \colon S' \to \nats$ by defining $g(s) = f(s)$ for all $s \in S'$.
\end{proof}

\begin{example}
  Some other examples of countable sets are: the set of even numbers, odd numbers, and prime numbers. These are all subsets of the natural numbers.
\end{example}

Notice that while there is an injection from the set of even numbers to the set of naturals, there is also an injection from the set of naturals to the set of even numbers. We will formalise this notion in the following claim.

\begin{claim}
  If $A$ and $B$ are countably infinite sets, then there is a bijection between $A$ and $B$. Thus, $|A| = |B|$.
\end{claim}

An even more important result is the following claim:

\begin{claim}
  \claimlabel{cartesian-countable}
  Let $A$ and $B$ be countably infinite sets. Then, $A \times B = \{(a, b) \mid a \in A, b \in B\}$ is countable.
\end{claim}

The idea of a proof for \claimref{cartesian-countable} is to draw a grid and list the elements of $A$ along the rows and the elements of $B$ along the columns. Numbering the elements along the diagonals of the grid, we can then define an injection from $A \times B$ to $\nats$.

\claimref{cartesian-countable} yields the following corollaries:

\begin{corollary}
  \corlabel{cartesian-nats-countable}
  $\nats \times \nats$ is countable.
\end{corollary}

\begin{corollary}
  $\rationals$ is countable.
\end{corollary}

\begin{proof}[Proof sketch]
  Every rational $r \in \rationals$ is of the form $p/q$ where $p, q \in \nats$. Thus, $r$ can be represented as a pair $(p, q) \in \nats \times \nats$ and so $\rationals$ is isomorphic to a subset of $\nats \times \nats$. By \corref{cartesian-nats-countable} and \claimref{subset-countable}, $\rationals$ is countable.
\end{proof}

\begin{corollary}
  $\integers$ is countable.
\end{corollary}

\begin{proof}[Proof sketch]
  For every integer $i$, we can write it as either $(0, i)$ or $(-1, i)$. Thus, $\integers$ is isomorphic to a subset of $\nats \times \nats$. By \corref{cartesian-nats-countable} and \claimref{subset-countable}, $\rationals$ is countable.
\end{proof}

\begin{theorem}
  \thmlabel{uncountable-countable}
  There is a collection $\conn$ of countable sets such that $\conn$ is uncountable.
\end{theorem}

\begin{example}
  The powerset of the natural numbers $\powerset{\nats}$ is an uncountable collection of countable sets.
\end{example}

Next, we state a more important observation:

\begin{theorem}
  Let $\conn$ be a collection of countable sets such that $\conn$ is countable. Then, $\euscr{I} = \bigcup_{S \in \conn}{S}$ is countable.
\end{theorem}

\begin{proof}[Proof sketch]
  We can write each element $c \in \euscr{I}$ as $(i, j)$ where $i$ is the index of the set $S \in \conn$ and $j$ is the index of the element $x \in S$. Thus, $\euscr{I}$ is isomorphic to some subset of $\nats \times \nats$. By \corref{cartesian-nats-countable} and \claimref{subset-countable}, $\rationals$ is countable.
\end{proof}

Finally, we arrive at one of the most important theorems in countability.

\begin{theorem}[Uncountability of reals]
  \thmlabel{uncountable-reals}
  $\reals$ is uncountable.
\end{theorem}

\begin{proof}[Proof sketch]
  The proof is by Cantor's diagonal argument.
\end{proof}

\begin{theorem}[Uncountability of powerset of naturals]
  $\powerset{\nats}$ is uncountable.
\end{theorem}

\begin{proof}[Proof sketch]
  For any set $S \in \powerset{\nats}$, we can associate a unique real number $r$ to $S$ where $r = 0.\ldots$ where the $i^{th}$ decimal place is $1$ if the $i^{th}$ natural number is in $S$ and $0$ otherwise. Hence, $\powerset{\nats}$ is isomorphic to $\reals$. By \thmref{uncountable-reals}, $\powerset{\nats}$ is uncountable.
\end{proof}

Next, let's look at alphabets, strings, and languages.

\begin{theorem}
  Let $\Sigma$ be some countable alphabet (or set). $\Sigma^*$ is countable, where $\Sigma^*$ denotes the set of all finite strings over $\Sigma$.
\end{theorem}

\begin{theorem}
  If $S$ be a countable set, then the set $\mathscr{P}_{\text{fin}}{(S)}$ of finite subsets of $S$ is countable.
\end{theorem}

We have previously seen that every set of strings on a countable alphabet is countable. But what about the set of all languages?

\begin{question}
  Let $\Sigma$ be a countable alphabet. A language $L$ over $\Sigma$ is a subset of $\Sigma^*$. Is the collection of all languages over $\Sigma$ countable? What if $\Sigma$ is finite?
\end{question}

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Primer on Computability Theory}
\noindent

Computability Theory asks questions like "what is the complexity of solving a problem?", or more generally, "is the problem solvable?". We will study computability theory in the context of first-order logic (FOL), by modelling computational problems as formulae and determining if such formulae are satisfiable in polynomial time.

Here's an exercise: write a program $P$ that takes
\begin{enumerate}
  \item a program $Q$ as input,
  \item and an input $I$ to $Q$ as input.
\end{enumerate}
such that $P$ outputs "yes" if $Q$ on input $I$ prints "hello" as the first 5 characters, and "no" otherwise.

It turns out that it is impossible to write such a program.

\begin{claim}
  There is no program $P$ that takes as input (1) a program $Q$ and (2) an input $I$ to $Q$ such that $P$ outputs "yes" if $Q$ on input $I$ prints "hello" as the first 5 characters, and "no" otherwise.
\end{claim}

\begin{proof}
  Suppose, on the contrary, that there exists such a program $P$. Then, we can write a program $P'$ which takes in a program $Q$ and runs $P(Q, Q)$, then outputs "no" if $P(Q, Q)$ outputs "yes" else it outputs "no".

  Now, suppose we execute $P'(P')$. If it outputs "no", then $P(P', P')$ outputs "yes", which means that $P'(P')$ outputs "hello". This is a contradiction. Otherwise, if $P'(P')$ outputs "hello", then $P(P', P')$ outputs "no", which means that $P'(P')$ does not output "hello". This is also a contradiction.

  In either case, we arrive at a contradiction. Therefore, there is no such program $P$.
\end{proof}

The previous claim shows that certain computational problems that are unsolvable. Another well-known unsolvable problem is the Halting problem, which is the problem of determining whether a given program will terminate on a given input.

\vspace{5truemm}
\hrule
\hrule

\section*{\large \centering Turing Machines}
\noindent

We use Turing machines as our main model of computation. Input that is fed into a Turing machine can be modelled as strings over an alphabet. We will define some terms and notation before proceeding.

\begin{definition}[Turing machine]
  A \emph{Turing machine} is a tuple $M = (Q, q_0, q\textsubscript{acc}, q\textsubscript{rej}, k, \delta, \Sigma)$ where
  \begin{enumerate}
    \item $Q$ is a finite set of control states
    \item $\Sigma$ is the alphabet of tape symbols
    \item $q_0 \in Q$ is the initial state
    \item $q\textsubscript{acc} \in Q$ is the accepting state
    \item $q\textsubscript{rej} \in Q$ is the rejecting state
  \end{enumerate}
\end{definition}

\begin{definition}[Configuration]
  A \emph{configuration} $C$ of a Turing machine $M$ is $C = (q, w\textsubscript{inp} \uparrow w'\textsubscript{inp}, w_{\text{WT}_1} \uparrow w'_{\text{WT}_1}, \ldots, w_{\text{WT}_k} \uparrow w'_{\text{WT}_k}, w\textsubscript{out} \uparrow w'\textsubscript{out})$ where $w_x$ are finite strings over $\Sigma$ representing the finite contents of the unbounded tape and $\uparrow$ refers to the pointer of the input tape, work tapes, and output tapes.
\end{definition}

\begin{definition}[Run/Computation]
  A \emph{run/computation} of Turing machine $M$ on input $w \in \Sigma^*$ is $\pi = c_0, c_1, \ldots, c_m$ such that $c_0$ is the initial configuration and, for each $i$, $c_{i+1}$ follows from $c_i$ using $\delta$.
\end{definition}

\begin{definition}[Accepting Run/Acceptance]
  A run $\pi$ is an \emph{accepting run} if there is a configuration $c$ in the run such that $c = (q\textsubscript{acc}, \ldots)$ and $q\textsubscript{rej}$ is not reached before $c$. The input $w$ is \emph{accepted} by $M$ iff the run $\pi$ on $M$ is an accepting run. Otherwise, $w$ is rejected.
\end{definition}

\begin{definition}[Language of a Turing Machine]
  The \emph{language} of a Turing machine $M$ is $L(M) = \{w \in \Sigma^* \mid w \text{ is accepted by $M$}\}$. A language $A \subseteq \Sigma^*$ is \emph{recognized/accepted} by $M$ if $A = L(M)$.
\end{definition}

\begin{definition}[Halting]
  A Turing machine $M$ \emph{halts} on input $w$ if there is a computation $\pi = c_0, c_1, \ldots, c_m$ such that $c_m = (q\textsubscript{acc}, \ldots)$ or $c_m = (q\textsubscript{rej}, \ldots)$. The run $\pi$ is called a \emph{halting run}.
\end{definition}

For a list of objects $O_1, O_2, \ldots, O_k$, we will use $\langle O_1, O_2, \ldots, O_k \rangle$ to denote their binary encoding. In particular, for a Turing machine $M$, $\langle M \rangle$ is its encoding as a binary string. We may then define the language $L\textsubscript{Halt} = \{\langle M, w \rangle \mid w \text{ on } M \text{ halts}\}$ Is there a Turing machine $H$ such that $L(H) = L\textsubscript{Halt}$?

The answer is yes: just simulate $M$ on $w$. This Turing machine is known as a \emph{universal Turing machine} since it can simulate the specification of an arbitrary Turing machine on arbitrary input.

\section*{\large \centering Recursive and Recursively Enumerable Languages}
\noindent

Recall that there are $3$ possible outcomes when a Turing machine $M$ runs on an input string $w$ --- $M$ may halt and accept $w$, $M$ may halt and reject $w$, or $M$ may not halt on $w$. Depending on how a Turing machine behaves,  we can define two different classes of problems solvable on a Turing machine.

\begin{definition}[Recursively Enumerable]
  A language $A$ is \emph{recursively enumerable/semi-decidable} if there is a Turing machine $M$ such that $A = L(M)$. We denote the set of all recursively enumerable languages as $\textsf{RE}$.
\end{definition}

\begin{example}
  $L\textsubscript{Halt}$ is recursively enumerable. That is, $L\textsubscript{Halt} \in \textsf{RE}$.
\end{example}

\begin{definition}[Recursive/Decidable]
  A language $A$ is \emph{recursive/decidable} if there is a Turing machine $M$ that halts on \emph{all} inputs and $A = L(M)$. We denote the set of all recursive languages as $\textsf{REC}$.
\end{definition}

As an example, consider the language $L\textsubscript{Sorted} = \{\langle l \rangle \mid l \text{ is a sorted list}\}$.

\begin{example}
  $L\textsubscript{Sorted} \in \textsf{REC}$ but $L\textsubscript{Halt} \not\in \textsf{REC}$.
\end{example}

Observe that a problem that is recursive is solvable by an algorithm that always halts. Thus, by definition, recursive languages are also recursively enumerable. This observation is equivalent to the following lemma:

\begin{lemma}[Recursive Implies Recursively Enumerable]
  \lemlabel{rec-implies-re}
  \textsf{REC} $\subseteq$ \textsf{RE}.
\end{lemma}

We also define the complement of a language.

\begin{definition}[Complement of a Language]
  The complement of a language $L$ is $\overline{L} = \Sigma^* \setminus L$.
\end{definition}

\begin{theorem}[Complement of Recursive is Recursive]
  If $L \in \textsf{REC}$, then $\overline{L} \in \textsf{REC}$.
\end{theorem}

\begin{proof}[Proof sketch]
  Take the Turing machine $M$ that accepts $L$ and let $M'$ be the Turing machine $M$ with the only difference being that the accepting and rejecting states are swapped. Then, $\overline{L} = L(M')$ and $M'$ halts on every input. Thus, $\overline{L} \in \textsf{REC}$.
\end{proof}

The following theorem is a useful way to prove that a problem is recursive.

\begin{theorem}
  \thmlabel{re-rec-link}
  $L$ is recursive iff $L$ and $\overline{L}$ are recursively enumerable. That is, $L \in \textsf{REC}$ iff $L \in \textsf{RE}$ and $\overline{L} \in \textsf{RE}$.
\end{theorem}

\begin{proof}[Proof sketch]
  The (easy) forward direction uses \lemref{rec-implies-re}.
  
  The reverse direction uses a technique called \emph{dovetailing}. Suppose that $L$ and $\overline{L}$ are recognized by $M$ and $\overline{M}$ respectively. Then, construct $M'$ by running both $M$ and $\overline{M}$ simultaneously on an input $w$, and accept if $M$ accepts or reject if $\overline{M}$ accepts. Since $w$ belongs to either $L$ or $\overline{L}$, $M'$ halts on all inputs and $L = L(M')$. Thus, $L \in \textsf{REC}$.
\end{proof}

Not every decision problem is recursively enumerable.

\begin{theorem}[Language outside \textsf{RE}]
  There is a language $L$ that is not recursively enumerable. That is, $L \not\in \textsf{RE}$.
\end{theorem}

\begin{proof}
  Since $L\textsubscript{Halt} \not\in \textsf{REC}$ but $L\textsubscript{Halt} \in \textsf{RE}$, by \thmref{re-rec-link}, $\overline{L\textsubscript{Halt}} \not\in \textsf{RE}$.

  Alternatively, note that there are uncountably many languages yet countably many Turing machines.
\end{proof}

Next, consider the language $K = \{\langle M \rangle \mid \langle M \rangle \in L(M)\}$. Using Cantor's diagonalization technique, we can establish that the complement $\overline{K}$ is not recursively enumerable.

\begin{theorem}
  The language $\overline{K} = \{\langle M \rangle \mid \langle M \rangle \not\in L(M)\}$ is not recursively enumerable. That is, $\overline{K} \notin \textsf{RE}$.
\end{theorem}

\begin{proof}
  Suppose to the contrary that $\overline{K} = L(M)$ for some Turing machine $M$. If $\langle M \rangle \in \overline{K}$, then $\langle M \rangle \not\in L(M) = \overline{K}$, which is a contradiction. Otherwise, if $\langle M \rangle \not\in \overline{K}$, then $\langle M \rangle \in L(M) = \overline{K}$, which is also a contradiction. Therefore, $\overline{K} \ne L(M)$ for any Turing machine $M$ and so $\overline{K} \notin \textsf{RE}$.
\end{proof}

\end{document}
